{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Recurrent State Space Model\n",
    "* Original paper: Variational Temporal Abstraction (https://arxiv.org/pdf/1910.00775.pdf)\n",
    "* Original code: https://github.com/taesupkim/vta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Recurrent State Space Model summary\n",
    "HRSSM infer the latent temporal structure and thus perform the stochastic state transition hierarchically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixyz\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 1\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Setting\n",
    "# generate MNIST by stacking row images(consider row as time step)\n",
    "def init_dataset(f_batch_size):\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "    data_dir = '../data'\n",
    "    mnist_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda data: data[0])\n",
    "    ])\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(data_dir, train=True, download=True,\n",
    "                       transform=mnist_transform),\n",
    "        batch_size=f_batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(data_dir, train=False, transform=mnist_transform),\n",
    "        batch_size=f_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    fixed_t_size = 28\n",
    "    return train_loader, test_loader, fixed_t_size\n",
    "\n",
    "train_loader, test_loader, t_max = init_dataset(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixyz.utils import print_latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the original paper\n",
    "### Generative model\n",
    "Generative process:  \n",
    "$p(X, Z, S, M)=\\prod_{t=1}^{T} p\\left(x_{t} | s_{t}\\right) p\\left(m_{t} | s_{t}\\right) p\\left(s_{t} | s_{<t}, z_{t}, m_{t-1}\\right) p\\left(z_{t} | z_{<t}, m_{t-1}\\right)$  \n",
    "\n",
    "Prior on Temporal Structure:  \n",
    "$p(m_t | s_t) = \\cal B(\\sigma(f_{m-mlp}(s_t))$  \n",
    "$p\\left(m_{t}=1 | s_{t}\\right)=\\left\\{\\begin{array}{ll}{0} & {\\text { if } n\\left(m_{<t}\\right) \\geq N_{\\max }} \\\\ {1} & {\\text { elseif } l\\left(m_{<t}\\right) \\geq l_{\\max }} \\\\ {\\sigma\\left(f_{m-\\operatorname{mlp}}\\left(s_{t}\\right)\\right)} & {\\text { otherwise }}\\end{array}\\right.$  \n",
    "\n",
    "\n",
    "Hierarchical Transitions:  \n",
    "$p\\left(z_{t} | z_{<t}, m_{<t}\\right)=\\left\\{\\begin{array}{ll}{\\delta\\left(z_{t}=z_{t-1}\\right)} & {\\text { if } m_{t-1}=0(\\mathrm{COPY})} \\\\ {\\tilde{p}\\left(z_{t} | c_{t}\\right)} & {\\text { otherwise (UPDATE) }}\\end{array}\\right.$  \n",
    "${\\tilde{p}\\left(z_{t} | c_{t}\\right)} = \\cal N(z_t | \\mu_z(c_t), \\sigma_z(c_t))$  \n",
    "\n",
    "$c_{t}=\\left\\{\\begin{array}{ll}{c_{t-1}} & {\\text { if } m_{t-1}=0(\\mathrm{COPY})} \\\\ {f_{z-\\text { rnn }}\\left(z_{t-1}, c_{t-1}\\right)} & {\\text { otherwise (UPDATE) }}\\end{array}\\right.$  \n",
    "\n",
    "\n",
    "Observation transition:  \n",
    "$p\\left(s_{t} | s_{<t}, z_{t}, m_{<t}\\right)=\\tilde{p}\\left(s_{t} | h_{t}\\right) \\quad \\text { where } \\quad h_{t}=\\left\\{\\begin{array}{ll}{f_{s-\\text { rnn }}\\left(s_{t-1} \\| z_{t}, h_{t-1}\\right)} & {\\text { if } m_{t-1}=0 \\text { (UPDATE) }} \\\\ {f_{s-\\text { mlp }}\\left(z_{t}\\right)} & {\\text { otherwise (INIT) }}\\end{array}\\right.$\n",
    "\n",
    "${\\tilde{p}\\left(s_{t} | h_{t}\\right)} = \\cal N(s_t | \\mu_z(h_t), \\sigma_z(h_t))$  \n",
    "\n",
    "### Inference\n",
    "$q(Z, S, M | X)=q(M | X) q(Z | M, X) q(S | Z, M, X)$\n",
    "\n",
    "Sequence Decomposition:  \n",
    "$q(M | X)=\\prod_{t=1}^{T} q\\left(m_{t} | X\\right)=\\prod_{t=1}^{T} \\operatorname{Bern}\\left(m_{t} | \\sigma(\\varphi(X))\\right)$  \n",
    "\n",
    "State Inference:  \n",
    "$q(Z | M, X)=\\prod_{t=1}^{T} q\\left(z_{t} | M, X\\right)$  \n",
    "\n",
    "$q\\left(z_{t} | M, X\\right)=\\left\\{\\begin{array}{ll}{\\delta\\left(z_{t}=z_{t-1}\\right)} & {\\text { if } m_{t-1}=0(\\mathrm{COPY})} \\\\ {\\tilde{q}\\left(z_{t} | \\psi_{t-1}^{\\mathrm{fwd}}, \\psi_{t}^{\\mathrm{bwd}}\\right)} & {\\text { otherwise (UPDATE) }}\\end{array}\\right.$\n",
    "\n",
    "Observation abstraction predictor:  \n",
    "$q(S | Z, M, X)=\\prod_{t=1}^{T} q\\left(s_{t} | z_{t}, M, X\\right)$  \n",
    "$q\\left(s_{t} | z_{t}, M, X\\right)=\\tilde{q}\\left(s_{t} | z_{t}, \\phi_{t}^{\\mathrm{fwd}}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixyz.distributions import Bernoulli, Normal, Deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractionExtractor(nn.Module):\n",
    "    def __init__(self, c_dim, z_dim, abs_feat_size):\n",
    "        super(AbstractionExtractor, self).__init__()\n",
    "        self.c_dim = c_dim\n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "        self.abs_feat_size = abs_feat_size\n",
    "        \n",
    "        self.fc = nn.Linear(self.c_dim + self.z_dim, self.abs_feat_size)\n",
    "    \n",
    "    def forward(self, c_dim, z_dim):\n",
    "        concat_feature = torch.cat((c_dim, z_dim), dim=-1)\n",
    "        return self.fc(concat_feature)\n",
    "\n",
    "\n",
    "class ObservationExtractor(nn.Module):\n",
    "    def __init__(self, h_dim, s_dim, obs_feat_size):\n",
    "        super(ObservationExtractor, self).__init__()\n",
    "        self.h_dim = h_dim\n",
    "        self.s_dim = s_dim\n",
    "        \n",
    "        self.obs_feat_size = obs_feat_size\n",
    "        \n",
    "        self.fc = nn.Linear(self.h_dim + self.s_dim, self.obs_feat_size)\n",
    "    \n",
    "    def forward(self, h_dim, s_dim):\n",
    "        concat_feature = torch.cat((h_dim, s_dim), dim=-1)\n",
    "        return self.fc(concat_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriorOnTemporalStructure(Bernoulli):\n",
    "    \"\"\"\n",
    "    Binary indicator\n",
    "    p(mt | st)\n",
    "    PriorBoundaryDetector\n",
    "    output_dimが2になっているのはなぜ？\n",
    "    nn.Identityで通している，あとでsigmoidかけているのか？\n",
    "    \"\"\"\n",
    "    def __init__(self, s_dim):\n",
    "        super(PriorOnTemporalStructure, self).__init__(cond_var=[\"s\"], var=[\"m\"])\n",
    "        self.s_dim = s_dim\n",
    "        self.fc1 = nn.Linear(s_dim, 1)\n",
    "        \n",
    "    def forward(self, s):\n",
    "        m = self.fc1(s)\n",
    "        m = torch.sigmoid(s)\n",
    "        return {\"probs\": m}\n",
    "\n",
    "\n",
    "class AbstRecurrence(Deterministic):\n",
    "    \"\"\"\n",
    "    RecurrentLayer\n",
    "    self.update_abs_beliefの部分\n",
    "    ct = f_z_rnn(z_t-1, c_t-1)\n",
    "    \"\"\"\n",
    "    def __init__(self, c_dim, z_dim):\n",
    "        super(Recurrence, self).__init__(cond_var=[\"c_prev\", \"z_prev\"], var=[\"c\"])\n",
    "        self.c_dim = c_dim\n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "        self.rnn_cell = nn.GRUCell(input_size=self.z_dim, hidden_size=self.c_dim)\n",
    "        \n",
    "    def forward(self, c_prev, z_prev):\n",
    "        c = self.rnn_cell(z_prev, c_prev)\n",
    "        return {\"c\": c}\n",
    "\n",
    "\n",
    "class ObsRecurrecne(Deterministic):\n",
    "    \"\"\"\n",
    "    RecurrentLayer\n",
    "    self.update_obs_beliefの部分\n",
    "    ht = f_s_rnn(s_t-1||zt, h_t-1)\n",
    "    \"\"\"\n",
    "    def __init__(self, s_dim, z_dim, h_dim):\n",
    "        super(ObsRecurrecne, self).__init__(cond_var=[\"s_prev\", \"z_prev\", \"h_prev\"], var=[\"h\"])\n",
    "        self.s_dim = s_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.h_dim = h_dim\n",
    "        \n",
    "        self.rnn_cell = nn.GRUCell(input_size=self.s_dim + self.z_dim, hidden_size=self.h_dim)\n",
    "        \n",
    "    def forward(self, s_prev, z_prev, h_prev):\n",
    "        concat_feature = torch.cat((s_prev, z_prev), dim=-1)\n",
    "        h = self.rnn_cell(concat_feature, h_prev)\n",
    "        return {\"h\": h}a\n",
    "\n",
    "    \n",
    "class TransitionOfTemporalAbstraction(Normal):\n",
    "    \"\"\"\n",
    "    p(z_t | c_t) = N(zt | mu(ct), sigma(ct))\n",
    "    Latent Distribution\n",
    "    self.prior_abs_stateの部分\n",
    "    \"\"\"\n",
    "    def __init__(self, c_dim, z_dim):\n",
    "        super(TransitionOfTemporalAbstraction, self).__init__(cond_var=[\"c\"], var=[\"z\"])\n",
    "        self.c_dim = c_dim\n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(c_dim, c_dim)\n",
    "        \n",
    "        self.fc21 = nn.Linear(c_dim, z_dim)\n",
    "        self.fc22 = nn.Linear(c_dim, z_dim)\n",
    "    \n",
    "    def forward(self, c):\n",
    "        h = nn.ELU(self.fc1(c), inplace=True)\n",
    "        return {\"loc\": self.fc21(h), \"scale\": F.softplus(self.fc22(h))}\n",
    "\n",
    "    \n",
    "class TransitionOfObservation(Normal):\n",
    "    \"\"\"\n",
    "    p(st | ht)\n",
    "    self.prior_obs_state = LatentDistributionの部分\n",
    "    \"\"\"\n",
    "    def __init__(self, h_dim, s_dim):\n",
    "        super(TransitionOfObservation, self).__init__(cond_var=[\"h\"], var=[\"s\"])\n",
    "        self.h_dim = h_dim\n",
    "        self.s_dim = s_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(h_dim, h_dim)\n",
    "        \n",
    "        self.fc21 = nn.Linear(h_dim, s_dim)\n",
    "        self.fc22 = nn.Linear(h_dim, s_dim)\n",
    "        \n",
    "    def forward(self, h):\n",
    "        h_ = nn.ELU(self.fc1(h), inplace=True)\n",
    "        return {\"loc\": self.fc21(h_), \"scale\": F.softplus(self.fc22(h_))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$p(m|s)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior = PriorOnTemporalStructure(3)\n",
    "print_latex(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$p(z|c)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = TransitionOfTemporalAbstraction(3, 3)\n",
    "print_latex(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$p(s|h)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = TransitionOfObservation(3, 3)\n",
    "print_latex(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$p(c|c_{prev},z_{prev})$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = Recurrence(3, 3)\n",
    "print_latex(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.3'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixyz.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
