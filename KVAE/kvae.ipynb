{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalman VAE\n",
    "- Original paper: A Disentangled Recognition and Nonlinear DynamicsModel for Unsupervised Learning (https://arxiv.org/pdf/1710.05741.pdf)\n",
    "- Original code: https://github.com/simonkamronn/kvae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kalman VAE summary\n",
    ">KVAE disentangles two latent representations: an object’s representation, coming from a recognition model, and a latent state describing its dynamics. As a result, the evolution of the world can be imagined and missing data imputed, both without the need to generate high dimensional frames at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from pixyz.utils import print_latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f02a8092df0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 1\n",
    "seed = 1\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate MNIST by stacking row images(consider row as time step)\n",
    "def init_dataset(f_batch_size):\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "    data_dir = '../data'\n",
    "    mnist_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda data: data[0])\n",
    "    ])\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(data_dir, train=True, download=True,\n",
    "                       transform=mnist_transform),\n",
    "        batch_size=f_batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(data_dir, train=False, transform=mnist_transform),\n",
    "        batch_size=f_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    fixed_t_size = 28\n",
    "    return train_loader, test_loader, fixed_t_size\n",
    "\n",
    "train_loader, test_loader, t_max = init_dataset(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define probability distributions\n",
    "### In the original paper\n",
    "Prior: $p_{\\gamma}({\\bf a}|{\\bf u}) = {\\int p_{\\gamma}({\\bf a}|{\\bf z})p_{\\gamma}({\\bf z}|{\\bf u})d{\\bf z}}$ (equation (3) in the paper)\n",
    "\n",
    "Generator: $p_{\\theta}({\\bf x}|{\\bf a}) = \\prod_{t=1}^{T}p_{\\theta}(x_t | a_t)$  \n",
    ">$p_{\\theta}(x_t|a_t)$ is a deep neural network parameterized by θ, that emits either a factorized Gaussian or Bernoulli probability vector depending on the data type of $x_t$.\n",
    "\n",
    "Inference:  $q_{\\phi}({\\bf a}|{\\bf x}) = \\prod_{t=1}^{T}q_{\\phi}(a_t | x_t)$  \n",
    "$q_{\\phi}(a_t | x_t) = {\\cal N}(\\mu_{\\phi}(x_t), \\Sigma_{\\phi}(x_t))$\n",
    ">$q_{\\phi}(a_t|x_t)$ is a deep neural network that maps xt to the mean and the diagonal covariance of a Gaussian distribution.  \n",
    "\n",
    "LGSSM:  \n",
    "$p_{\\gamma_{t}}\\left(\\mathbf{z}_{t} | \\mathbf{z}_{t-1}, \\mathbf{u}_{t}\\right)=\\mathcal{N}\\left(\\mathbf{z}_{t} ; \\mathbf{A}_{t} \\mathbf{z}_{t-1}+\\mathbf{B}_{t} \\mathbf{u}_{t}, \\mathbf{Q}\\right), \\quad p_{\\gamma_{t}}\\left(\\mathbf{a}_{t} | \\mathbf{z}_{t}\\right)=\\mathcal{N}\\left(\\mathbf{a}_{t} ; \\mathbf{C}_{t} \\mathbf{z}_{t}, \\mathbf{R}\\right)$ (equation (1) in the paper)\n",
    "\n",
    "$p_{\\gamma}(\\mathbf{a}, \\mathbf{z} | \\mathbf{u})=\\prod_{t=1}^{T} p_{\\gamma_{t}}\\left(\\mathbf{a}_{0: t-1}\\right)\\left(\\mathbf{a}_{t} | \\mathbf{z}_{t}\\right) \\cdot p\\left(\\mathbf{z}_{1}\\right) \\prod_{t=2}^{T} p_{\\gamma_{t}\\left(\\mathbf{a}_{0: t-1}\\right)}\\left(\\mathbf{z}_{t} | \\mathbf{z}_{t-1}, \\mathbf{u}_{t}\\right)$ (equation (8) in the paper)\n",
    "\n",
    "dynamics parameter network: \n",
    "${\\bf \\alpha}_t = {\\bf \\alpha}_t(a_{0:t-1})$  \n",
    "${\\bf d}_t = LSTM(a_{t-1}, d_{t-1})$  \n",
    "${\\bf \\alpha}_t = softmax({\\bf d}_t)$\n",
    "$\\mathbf{A}_{t}=\\sum_{k=1}^{K} \\alpha_{t}^{(k)}\\left(\\mathbf{a}_{0: t-1}\\right) \\mathbf{A}^{(k)}, \\quad \\mathbf{B}_{t}=\\sum_{k=1}^{K} \\alpha_{t}^{(k)}\\left(\\mathbf{a}_{0: t-1}\\right) \\mathbf{B}^{(k)}, \\quad \\mathbf{C}_{t}=\\sum_{k=1}^{K} \\alpha_{t}^{(k)}\\left(\\mathbf{a}_{0: t-1}\\right) \\mathbf{C}^{(k)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/simonkamronn/kvae/blob/849d631dbf2faf2c293d56a0d7a2e8564e294a51/kvae/KalmanVariationalAutoencoder.py#L97\n",
    "\n",
    "from pixyz.distributions import Normal\n",
    "class Inference(Normal):\n",
    "    def __init__(self, x_dim, a_dim):\n",
    "        super(Inference, self).__init__(name=\"q_phi\", cond_var=[\"x_t\"], var=[\"a_t\"])\n",
    "        self.fc1 = nn.Linear(x_dim, 25)\n",
    "        self.fc2 = nn.Linear(25, 25)\n",
    "        \n",
    "        self.fc3_1 = nn.Linear(25, a_dim)\n",
    "        self.fc3_2 = nn.Linear(25, a_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return {\"loc\": self.fc3_1(h), \"scale\": F.softplus(self.fc3_2(h))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$q_{\\phi}(a_{t}|x_{t})$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Inference(44, 44)\n",
    "print_latex(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/simonkamronn/kvae/blob/849d631dbf2faf2c293d56a0d7a2e8564e294a51/kvae/KalmanVariationalAutoencoder.py#L132\n",
    "\n",
    "from pixyz.distributions import Bernoulli\n",
    "class Generator(Bernoulli):\n",
    "    def __init__(self, a_dim, x_dim):\n",
    "        super(Generator, self).__init__(name=\"p_theta\", cond_var=[\"a_t\"], var=[\"x_t\"])\n",
    "        self.fc1 = nn.Linear(a_dim, 25)\n",
    "        self.fc2 = nn.Linear(25, 25)\n",
    "        \n",
    "        self.fc3 = nn.Linear(25, x_dim)\n",
    "    \n",
    "    def forward(self, a):\n",
    "        h = F.relu(self.fc1(a))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return {\"probs\": torch.sigmoid(self.fc3(h))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$p_{\\theta}(x_{t}|a_{t})$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Generator(44, 44)\n",
    "print_latex(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/simonkamronn/kvae/blob/849d631dbf2faf2c293d56a0d7a2e8564e294a51/kvae/KalmanVariationalAutoencoder.py#L176\n",
    "\n",
    "from pixyz.distributions import Deterministic\n",
    "class RNN(Deterministic):\n",
    "    def __init__(self, a_dim, d_dim):\n",
    "        super(RNN, self).__init__(name=\"LSTM\", cond_var=[\"a_prev\", \"d_prev\"], var=[\"d\"])\n",
    "        # d_dim = 50\n",
    "        self.rnn = nn.LSTMCell(a_dim, d_dim)\n",
    "        \n",
    "    def forward(self, a_prev, d_prev):\n",
    "        h, _ = self.rnn(a_prev, d_prev)\n",
    "        return {\"d\": h}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/simonkamronn/kvae/blob/849d631dbf2faf2c293d56a0d7a2e8564e294a51/kvae/KalmanVariationalAutoencoder.py#L220\n",
    "\n",
    "from pixyz.distributions import Deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$LSTM(d|a_{prev},d_{prev})$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = RNN(10, 50)\n",
    "print_latex(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/simonkamronn/kvae/blob/849d631dbf2faf2c293d56a0d7a2e8564e294a51/kvae/KalmanVariationalAutoencoder.py#L220\n",
    "\n",
    "from pixyz.distributions import Deterministic\n",
    "class DynamicParameterNetwork(Deterministic):\n",
    "    def __init__(self, d_dim, k_num):\n",
    "        super(DynamicParameterNetwork, self).__init__(name=\"p_alpha\", cond_var=[\"d\"], var=[\"alpha\"])\n",
    "        self.fc1 = nn.Linear(d_dim, k_num)\n",
    "        \n",
    "    def forward(self, d):\n",
    "        return {\"alpha\": F.softmax(self.fc1(d))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$p_{\\alpha}(\\alpha|d)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = DynamicParameterNetwork(50, 3)\n",
    "print_latex(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$p(\\alpha,d|a_{prev},d_{prev}) = p_{\\alpha}(\\alpha|d)LSTM(d|a_{prev},d_{prev})$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dynamic = a * r\n",
    "print_latex(Dynamic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss function\n",
    "$\\mathcal{F}(\\theta, \\gamma, \\phi)=\\mathbb{E}_{q_{\\phi}(\\mathbf{a} | \\mathbf{x})}\\left[\\log \\frac{p_{\\theta}(\\mathbf{x} | \\mathbf{a})}{q_{\\phi}(\\mathbf{a} | \\mathbf{x})}+\\mathbb{E}_{p_{\\gamma}(\\mathbf{z} | \\mathbf{a}, \\mathbf{u})}\\left[\\log \\frac{p_{\\gamma}(\\mathbf{a} | \\mathbf{z}) p_{\\gamma}(\\mathbf{z} | \\mathbf{u})}{p_{\\gamma}(\\mathbf{z} | \\mathbf{a}, \\mathbf{u})}\\right]\\right]$ (euqation (6) in the paper)\n",
    "\n",
    "$\\hat{\\mathcal{F}}(\\theta, \\gamma, \\phi)=\\frac{1}{I} \\sum_{i} \\log p_{\\theta}\\left(\\mathbf{x} | \\widetilde{\\mathbf{a}}^{(i)}\\right)+\\log p_{\\gamma}\\left(\\widetilde{\\mathbf{a}}^{(i)}, \\widetilde{\\mathbf{z}}^{(i)} | \\mathbf{u}\\right)-\\log q_{\\phi}\\left(\\widetilde{\\mathbf{a}}^{(i)} | \\mathbf{x}\\right)-\\log p_{\\gamma}\\left(\\widetilde{\\mathbf{z}}^{(i)} | \\widetilde{\\mathbf{a}}^{(i)}, \\mathbf{u}\\right)$ (euqation (7) in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
