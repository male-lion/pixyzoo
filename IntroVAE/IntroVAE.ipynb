{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixyz.distributions import Normal\n",
    "from pixyz.utils import print_latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_Block(nn.Module):\n",
    "    def __init__(self, in_c=64, out_c=64, groups=1, scale=1.0):\n",
    "        super(Residual_Block, self).__init__()\n",
    "        # (N, N, in_c) -> (N, N, out_c)\n",
    "        \n",
    "        mid_c = int(out_c * scale)\n",
    "        \n",
    "        if in_c is not out_c:\n",
    "            self.conv_expand = nn.Conv2d(in_channels=in_c, out_channels=out_c, kernel_size=1, stride=1, padding=0, groups=1, bias=False)\n",
    "        else:\n",
    "            self.conv_expand = None\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=in_c, out_channels=mid_c, kernel_size=3, stride=1, padding=1, groups=groups, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(mid_c)\n",
    "        self.relu1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.conv2 = nn.Conv2d(in_channels=mid_c, out_channels=out_c, kernel_size=3, stride=1, padding=1, groups=groups, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)\n",
    "        self.relu2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.conv_expand is not None:\n",
    "            identity_data = self.conv_expand(x)\n",
    "        else:\n",
    "            identity_data = x\n",
    "        \n",
    "        output = self.relu1(self.bn1(self.conv1(x)))\n",
    "        output = self.conv2(output)\n",
    "        output = self.relu2(torch.add(self.bn2(output), identity_data))\n",
    "        # output = self.relu2(self.bn2(torch.add(output,identity_data))) <- original code\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Normal):\n",
    "    def __init__(self, c_dim=3, h_dim=512, channels=[64, 128, 256, 512, 512, 512], image_size=256):\n",
    "        super(Encoder, self).__init__(cond_var=[\"x\"], var=[\"z\"])\n",
    "        # (image_size, ,image_size) -> (image_size//(2**len(channels)), image_size//(2**len(channels))\n",
    "        \n",
    "        assert (2 ** len(channels)) * 4 == image_size\n",
    "        \n",
    "        cc = channels[0]\n",
    "        self.main = nn.Sequential(\n",
    "            #(image_size, image_size, c_dim) -> (image_size, image_size, cc)\n",
    "            nn.Conv2d(c_dim, cc, kernel_size=5, stride=1, padding=2, bias=False),\n",
    "            nn.BatchNorm2d(cc),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # (image_size, image_size, cc) -> (image_size/2, image_size/2, cc)\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "        )\n",
    "        \n",
    "        sz = image_size // 2\n",
    "        for ch in channels[1:]:\n",
    "            self.main.add_module('res_in_{}'.format(sz), Residual_Block(in_c=cc, out_c=ch, scale=1.0))\n",
    "            self.main.add_module('down_to_{}'.format(sz//2), nn.AvgPool2d(kernel_size=2))\n",
    "            cc, sz = ch, sz//2\n",
    "        \n",
    "        self.main.add_module('res_in_{}'.format(sz), Residual_Block(in_c=cc, out_c=cc, scale=1.0))\n",
    "        # len(channels) = 6\n",
    "        # 256 / (2**6) = 4\n",
    "        self.fc = nn.Linear(cc*4*4, 2*h_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.main(x).view(x.size(0), -1) # (Batch, image_size, image_size, c_dim) -> (Batch_size, channels[0] * 4 *4)\n",
    "        y = self.fc(y) # (Batch_size, channels[0] * 4 *4) -> (Batch_size, 2*h_dim)\n",
    "        mu, logvar = y.chunk(2, dim=1)\n",
    "        return {\"loc\": mu, \"scale\": F.softplus(logvar)}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle p(z|x)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Encoder()\n",
    "print_latex(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Normal):\n",
    "    def __init__(self, c_dim=3, h_dim=512, channels=[64, 128, 256, 512, 512, 512], image_size=256):\n",
    "        super(Decoder, self).__init__(cond_var=[\"z\"], var=[\"x\"])\n",
    "        \n",
    "        assert (2 ** len(channels)) * 4 == image_size\n",
    "        cc = channels[-1]\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(h_dim, cc*4*4),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        sz = 4\n",
    "        \n",
    "        self.main = nn.Sequential()\n",
    "        for ch in channels[::-1]:\n",
    "            self.main.add_module('res_in_{}'.format(sz), Residual_Block(in_c=cc, out_c=ch, scale=1.0))\n",
    "            self.main.add_module('up_to_{}'.format(sz*2), nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "            cc, sz = ch, sz*2\n",
    "        \n",
    "        self.main.add_module('res_in_{}'.format(sz), Residual_Block(in_c=cc, out_c=cc, scale=1.0))\n",
    "        self.main.add_module('predict', nn.Conv2d(in_channels=cc, out_channels=c_dim, kernel_size=5, stride=1, padding=2))\n",
    "        \n",
    "    def forward(self, z):\n",
    "        z = z.view(z.size(0), -1) # (Batch, h_dim)\n",
    "        y = self.fc(z) # (Batch, h_dim) -> (Batch, cc*4*4)\n",
    "        y = y.view(z.size(0), -1, 4, 4) # (Batch, cc*4*4) -> (Batch, cc, 4, 4)\n",
    "        y = self.main(y) # (Batch, cc, 4, 4) -> (Batch, 3, 256, 256)\n",
    "        return {\"loc\": y, \"scale\": torch.ones(y.size())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle p(x|z)$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = Decoder()\n",
    "print_latex(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixyz.losses import KullbackLeibler, LogProb\n",
    "from pixyz.losses import Expectation as E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L_{E}(x, z)=D_{K L}\\left(q_{\\phi}(z | x) \\| p(z)\\right) +\\alpha * \\max (0, m-D_{K L}\\left(q_{\\phi}(z | G(z)) \\| p(z)\\right))- \\beta * E_{q_{\\phi}(z | x)} \\log p_{\\theta}(x | z)$\n",
    "\n",
    "$L_{G}(z)=\\alpha * D_{K L}\\left(q_{\\phi}(z | G(z)) \\| p(z)\\right)- \\beta * E_{q_{\\phi}(z | x)} \\log p_{\\theta}(x | z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input keys are not valid, expected ['x'] but got [].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-5232e5d2ef31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mkl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKullbackLeibler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.7.7/lib/python3.7/site-packages/pixyz/losses/losses.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, x_dict, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             raise ValueError(\"Input keys are not valid, expected {} but got {}.\".format(self._input_var,\n\u001b[0;32m--> 206\u001b[0;31m                                                                                         list(x_dict.keys())))\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input keys are not valid, expected ['x'] but got []."
     ]
    }
   ],
   "source": [
    "kl = KullbackLeibler(a, prior)\n",
    "kl.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input keys are not valid, expected ['x'] but got [].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-d2f43e2c152f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrecon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogProb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrecon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.7.7/lib/python3.7/site-packages/pixyz/losses/losses.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, x_dict, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             raise ValueError(\"Input keys are not valid, expected {} but got {}.\".format(self._input_var,\n\u001b[0;32m--> 206\u001b[0;31m                                                                                         list(x_dict.keys())))\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input keys are not valid, expected ['x'] but got []."
     ]
    }
   ],
   "source": [
    "recon = E(a, LogProb(d))\n",
    "recon.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixyz.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntroVAE(Model):\n",
    "    def __init__(self, c_dim=3, h_dim=512, channels=[64, 128, 256, 512, 512, 512], image_size=256):\n",
    "        super(IntroVAE, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(c_dim, h_dim, channels, image_size)\n",
    "        self.decoder = Decoder(c_dim, h_dim, channels, image_size)\n",
    "        self.prior = Normal(loc=torch.tensor(0.), scale=torch.tensor(1.),\n",
    "               var=[\"z\"], features_shape=[h_dim], name=\"p_{prior}\")\n",
    "        \n",
    "        self.kl_loss = KullbackLeibler(self.encoder, self.prior)\n",
    "        self.recon_loss = E(self.encoder, LogProb(self.decoder))\n",
    "        \n",
    "        self.vae_loss = (self.kl_loss - self.recon_loss).mean()\n",
    "        \n",
    "        self.m_plus = 0\n",
    "        self.weight_neg = 0\n",
    "        self.weight_rec = 0\n",
    "        self.weight_kl = 0\n",
    "        \n",
    "        self.lr_e = 1e-3\n",
    "        self.lr_g = 1e-3\n",
    "        \n",
    "        self.optimizerE = optim.Adam(self.encoder.parameters(), lr=self.lr_e)\n",
    "        self.optimizerG = optim.Adam(self.decoder.parameters(), lr=self.lr_g)\n",
    "        \n",
    "        distributions = [self.encoder, self.decoder]\n",
    "        self.distributions = nn.ModuleList(distributions)\n",
    "    \n",
    "    def calculate_vae_loss(real):\n",
    "        vae_loss = self.vae_loss.eval({\"x\": real})\n",
    "        return vae_loss\n",
    "    \n",
    "    def calculate_intro_loss(rec, fake, real):\n",
    "        # Encoder Loss\n",
    "        loss_rec = self.recon_loss.eval({\"x\": real}).mean()\n",
    "        \n",
    "        lossE_real_kl = self.kl_loss.eval({\"x\": real}).mean()\n",
    "        lossE_rec_kl = self.kl_loss.eval({\"x\": rec.detach()}).mean()\n",
    "        lossE_fake_kl = self.kl_loss.eval({\"x\": fake.detach()}).mean()\n",
    "        loss_margin = lossE_real_kl + \\\n",
    "                      (F.relu(self.m_plus-lossE_rec_kl) + \\\n",
    "                      F.relu(self.m_plus-lossE_fake_kl)) * 0.5 * self.weight_neg\n",
    "        lossE = loss_rec  *self.weight_rec + loss_margin * self.weight_kl\n",
    "        \n",
    "        # Generator Loss\n",
    "        lossG_rec_kl = self.kl_loss.eval({\"x\": rec}).mean()\n",
    "        lossG_fake_kl = self.kl_loss.eval({\"x\": fake}).mean()\n",
    "        lossG = (lossG_rec_kl + lossG_fake_kl)* 0.5 * self.weight_kl\n",
    "        \n",
    "        return lossE, lossG\n",
    "    \n",
    "    def train(self, train_x_dixt={}, vae=False):\n",
    "        self.distributions.train()\n",
    "        \n",
    "        if vae:\n",
    "            vae_loss = calculate_vae_loss(real=train_x_dixt[\"x\"])\n",
    "            \n",
    "            self.optimizerG.zero_grad()\n",
    "            self.optimizerE.zero_grad()       \n",
    "            vae_loss.backward()                   \n",
    "            self.optimizerE.step() \n",
    "            self.optimizerG.step()\n",
    "            \n",
    "            return vae_loss.item()\n",
    "        \n",
    "        else:\n",
    "            real = train_x_dixt[\"x\"]\n",
    "            fake = (self.prior * self.decoder).sample()[\"x\"]\n",
    "            z = self.encoder.sample(train_x_dixt, return_all=False, reparam=True)\n",
    "            # sample_mean() ???\n",
    "            rec = self.decoder.sample({\"z\": z}, return_all=False)\n",
    "            lossE, lossG = calculate_intro_loss(rec, fake, real)\n",
    "            \n",
    "            # update Encoder\n",
    "            self.optimizerE.zero_grad()       \n",
    "            lossE.backward()\n",
    "            self.optimizerE.step()\n",
    "            \n",
    "            # update Decoder\n",
    "            self.optimizerG.zero_grad()       \n",
    "            lossG.backward()\n",
    "            self.optimizerG.step()\n",
    "            return lossE.item(), lossG.item()\n",
    "        \n",
    "    def test(self, train_x_dixt={}, vae=False):\n",
    "        self.distributions.eval()\n",
    "        with torch.no_grad():\n",
    "            if vae:\n",
    "                vae_loss = calculate_vae_loss(real=train_x_dixt[\"x\"])\n",
    "                return vae_loss.item()\n",
    "\n",
    "            else:\n",
    "                real = train_x_dixt[\"x\"]\n",
    "                fake = (self.prior * self.decoder).sample()[\"x\"]\n",
    "                z = self.encoder.sample(train_x_dixt, return_all=False, reparam=True)\n",
    "                # sample_mean() ???\n",
    "                rec = self.decoder.sample({\"z\": z}, return_all=False)\n",
    "                lossE, lossG = calculate_intro_loss(rec, fake, real)\n",
    "                return lossE.item(), lossG.item()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
