{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f77ddc5adf0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 5\n",
    "seed = 1\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Setting\n",
    "# generate MNIST by stacking row images(consider row as time step)\n",
    "def init_dataset(f_batch_size):\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "    data_dir = '../data'\n",
    "    mnist_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda data: data[0])\n",
    "    ])\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(data_dir, train=True, download=True,\n",
    "                       transform=mnist_transform),\n",
    "        batch_size=f_batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(data_dir, train=False, transform=mnist_transform),\n",
    "        batch_size=f_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    fixed_t_size = 28\n",
    "    return train_loader, test_loader, fixed_t_size\n",
    "\n",
    "train_loader, test_loader, t_max = init_dataset(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixyz.models import Model\n",
    "from pixyz.losses import KullbackLeibler, CrossEntropy, IterativeLoss\n",
    "from pixyz.distributions import Bernoulli, Normal, Deterministic\n",
    "from pixyz.utils import print_latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(Deterministic):\n",
    "    '''\n",
    "    push the observed x through the rnn\n",
    "    rnn output contains the hidden state at each time step\n",
    "    '''\n",
    "    def __init__(self, x_dim, rnn_dim):\n",
    "        super(RNN, self).__init__(cond_var=[\"x\"], var=[\"h\"])\n",
    "        self.rnn = nn.GRU(x_dim, rnn_dim, bidirectional=True)\n",
    "#         self.h0 = torch.zeros(2, batch_size, self.rnn.hidden_size).to(device)\n",
    "        self.h0 = nn.Parameter(torch.zeros(2, 1, self.rnn.hidden_size))\n",
    "        self.hidden_size = self.rnn.hidden_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # if on gpu we need the fully broadcast view of the rnn initial state\n",
    "        # to be in contiguous gpu memory\n",
    "        h0 = self.h0.expand(2, x.size(1), self.rnn.hidden_size).contiguous()\n",
    "        h, _ = self.rnn(x, h0)\n",
    "        return {\"h\": h}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(Bernoulli):\n",
    "    '''\n",
    "    Emitter\n",
    "    Parameterizes the bernoulli observation likelihood p(x_t | z_t)\n",
    "    '''\n",
    "    def __init__(self, z_dim, hidden_dim):\n",
    "        super(Generator, self).__init__(cond_var=[\"z\"], var=[\"x\"])\n",
    "        # initialize the two linear transformations used in the neural network\n",
    "        self.lin_z_to_hidden = nn.Linear(z_dim, hidden_dim)\n",
    "        self.lin_hidden_to_input = nn.Linear(hidden_dim, x_dim)\n",
    "\n",
    "        # initialize the two non-linearities used in the neural network\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, z):\n",
    "        '''\n",
    "        Given the latent z at a particular time step t, return the vector of\n",
    "        probabilities taht parameterizes the bernlulli distribution p(x_t | x_t)\n",
    "        '''\n",
    "        h1 = self.relu(self.lin_z_to_hidden(z))\n",
    "        probs = self.sigmoid(self.lin_hidden_to_input(h1))\n",
    "        return {\"probs\": probs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inference(Normal):\n",
    "    '''\n",
    "    Combiner\n",
    "    Parameterizes q(z_t | z_{t-1}, x_{t:T}), which is the basic building block\n",
    "    of te guide(i.e. the variational distribution). The dependence on x_{t:T} is\n",
    "    through the hidden state of the RNN\n",
    "    '''\n",
    "    def __init__(self, z_dim, rnn_dim):\n",
    "        super(Inference, self).__init__(cond_var=[\"h\", \"z_prev\"], var=[\"z\"])\n",
    "        # initialize the three linear transformations used in the neural network\n",
    "        self.lin_z_to_hidden = nn.Linear(z_dim, rnn_dim*2)\n",
    "        self.lin_hidden_to_loc = nn.Linear(rnn_dim*2, z_dim)\n",
    "        self.lin_hidden_to_scale = nn.Linear(rnn_dim*2, z_dim)\n",
    "        # initialize the two non-linearities used in the neural network\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "        \n",
    "    def forward(self, h, z_prev):\n",
    "        '''\n",
    "        given the latent z at a particular time step t-1 as well as the hidden\n",
    "        state of the RNN h(x_{t:T}), return the mean and scale vectors that\n",
    "        parameterize the gaussian distribution q(z_t | z_{t-1}, x_{t:T})\n",
    "        '''\n",
    "        # combine the rnn hideen state with a trasnformed bersion of z_{t-1}\n",
    "        h_z = self.tanh(self.lin_z_to_hidden(z_prev))\n",
    "        h_combined = 0.5 * (h + h_z)\n",
    "\n",
    "        # use the combined hidden state to compute the mean used to sample z_t\n",
    "        loc = self.lin_hidden_to_loc(h_combined)\n",
    "        # use the combined hidden state to compute the scale used to sample z_t\n",
    "        scale = self.softplus(self.lin_hidden_to_scale(h_combined))\n",
    "        return {\"loc\": loc, \"scale\": scale}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prior(Normal):\n",
    "    '''\n",
    "    GatedTranstion\n",
    "    Parameterizes the gaussian latent transition probability p(z_t | z_{t-1})\n",
    "    '''\n",
    "    def __init__(self, z_dim, transition_dim):\n",
    "        super(Prior, self).__init__(cond_var=[\"z_prev\"], var=[\"z\"])\n",
    "        # initialize the 3 linear transformations used in the neural network\n",
    "        self.lin_gate_z_to_hidden = nn.Linear(z_dim, transition_dim)\n",
    "        self.lin_gate_hidden_to_z = nn.Linear(transition_dim, z_dim)\n",
    "\n",
    "        self.lin_proposed_mean_z_to_hidden = nn.Linear(z_dim, transition_dim)\n",
    "        self.lin_proposed_mean_hidden_to_z = nn.Linear(transition_dim, z_dim)\n",
    "\n",
    "        self.lin_sig = nn.Linear(z_dim, z_dim)\n",
    "        self.lin_z_to_loc = nn.Linear(z_dim, z_dim)\n",
    "\n",
    "        # modify the default initialization of lin_z_to_loc\n",
    "        # so that it's starts out as the identity function\n",
    "        self.lin_z_to_loc.weight.data = torch.eye(z_dim)\n",
    "        self.lin_z_to_loc.bias.data = torch.zeros(z_dim)\n",
    "\n",
    "        # initialize the 3 non-linearities used in the neural network\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "    def forward(self, z_prev):\n",
    "        '''\n",
    "        Given the latent z_{t-1} correspoding to the time step t-1\n",
    "        return the mean and scale vectors that parameterize the\n",
    "        gaussian distribution p(z_t | z_{t-1})\n",
    "        '''\n",
    "        # compute the gating function\n",
    "        _gate = self.relu(self.lin_gate_z_to_hidden(z_prev))\n",
    "        gate = self.sigmoid(self.lin_gate_hidden_to_z(_gate))\n",
    "\n",
    "        # compute the 'proposed mean'\n",
    "        _proposed_mean = self.relu(self.lin_proposed_mean_z_to_hidden(z_prev))\n",
    "        proposed_mean = self.lin_proposed_mean_hidden_to_z(_proposed_mean)\n",
    "\n",
    "        # assemble the actual mean used to sample z_t, which mixes\n",
    "        # a linear transformation of z_{t-1} with the proposed mean\n",
    "        # modulated by the gating function\n",
    "        # we don't want to force the dynamics be-nonlinear\n",
    "        loc = (1 - gate) * self.lin_z_to_loc(z_prev) + gate * proposed_mean\n",
    "        \n",
    "        # compute the scale used to sample z_t, using the proposed\n",
    "        # mean from above as input. the softplus ensures that scale is positive\n",
    "        # scale の元がmeanなの初めてみたよ\n",
    "        scale = self.softplus(self.lin_sig(self.relu(proposed_mean)))\n",
    "        \n",
    "        # return loc, scale which can be fed into Normal\n",
    "        return {\"loc\": loc, \"scale\": scale}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dim = 28\n",
    "hidden_dim = 32\n",
    "rnn_dim = hidden_dim * 2\n",
    "transition_dim = 32\n",
    "z_dim = 16\n",
    "t_max = x_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = Prior(z_dim=z_dim, transition_dim=transition_dim).to(device)# p(z_t| z_{t-1})\n",
    "encoder = Inference(z_dim=z_dim, rnn_dim=rnn_dim).to(device)#q(z_t | z_{t-1}, x_{t:T})\n",
    "decoder = Generator(z_dim=z_dim, hidden_dim=hidden_dim).to(device)# p(x_t|z_t)\n",
    "rnn = RNN(x_dim=x_dim, rnn_dim=rnn_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p(z|z_{prev})\n",
      "Network architecture:\n",
      "  Prior(\n",
      "    name=p, distribution_name=Normal,\n",
      "    var=['z'], cond_var=['z_prev'], input_var=['z_prev'], features_shape=torch.Size([])\n",
      "    (lin_gate_z_to_hidden): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (lin_gate_hidden_to_z): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (lin_proposed_mean_z_to_hidden): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (lin_proposed_mean_hidden_to_z): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (lin_sig): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (lin_z_to_loc): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (sigmoid): Sigmoid()\n",
      "    (softplus): Softplus(beta=1, threshold=20)\n",
      "  )\n",
      "********************************************************************************\n",
      "Distribution:\n",
      "  p(z|h,z_{prev})\n",
      "Network architecture:\n",
      "  Inference(\n",
      "    name=p, distribution_name=Normal,\n",
      "    var=['z'], cond_var=['h', 'z_prev'], input_var=['h', 'z_prev'], features_shape=torch.Size([])\n",
      "    (lin_z_to_hidden): Linear(in_features=16, out_features=128, bias=True)\n",
      "    (lin_hidden_to_loc): Linear(in_features=128, out_features=16, bias=True)\n",
      "    (lin_hidden_to_scale): Linear(in_features=128, out_features=16, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (softplus): Softplus(beta=1, threshold=20)\n",
      "  )\n",
      "********************************************************************************\n",
      "Distribution:\n",
      "  p(x|z)\n",
      "Network architecture:\n",
      "  Generator(\n",
      "    name=p, distribution_name=Bernoulli,\n",
      "    var=['x'], cond_var=['z'], input_var=['z'], features_shape=torch.Size([])\n",
      "    (lin_z_to_hidden): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (lin_hidden_to_input): Linear(in_features=32, out_features=28, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      "********************************************************************************\n",
      "Distribution:\n",
      "  p(h|x)\n",
      "Network architecture:\n",
      "  RNN(\n",
      "    name=p, distribution_name=Deterministic,\n",
      "    var=['h'], cond_var=['x'], input_var=['x'], features_shape=torch.Size([])\n",
      "    (rnn): GRU(28, 64, bidirectional=True)\n",
      "  )\n"
     ]
    }
   ],
   "source": [
    "print(prior)\n",
    "print(\"*\"*80)\n",
    "print(encoder)\n",
    "print(\"*\"*80)\n",
    "print(decoder)\n",
    "print(\"*\"*80)\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p(x,z|z_{prev}) = p(x|z)p(z|z_{prev})\n",
      "Network architecture:\n",
      "  Prior(\n",
      "    name=p, distribution_name=Normal,\n",
      "    var=['z'], cond_var=['z_prev'], input_var=['z_prev'], features_shape=torch.Size([])\n",
      "    (lin_gate_z_to_hidden): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (lin_gate_hidden_to_z): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (lin_proposed_mean_z_to_hidden): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (lin_proposed_mean_hidden_to_z): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (lin_sig): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (lin_z_to_loc): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (sigmoid): Sigmoid()\n",
      "    (softplus): Softplus(beta=1, threshold=20)\n",
      "  )\n",
      "  Generator(\n",
      "    name=p, distribution_name=Bernoulli,\n",
      "    var=['x'], cond_var=['z'], input_var=['z'], features_shape=torch.Size([])\n",
      "    (lin_z_to_hidden): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (lin_hidden_to_input): Linear(in_features=32, out_features=28, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$p(x,z|z_{prev}) = p(x|z)p(z|z_{prev})$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_from_prior = prior * decoder\n",
    "print(generate_from_prior)\n",
    "print_latex(generate_from_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_loss = CrossEntropy(encoder, decoder) + KullbackLeibler(encoder, prior)\n",
    "_loss = IterativeLoss(step_loss, max_iter=t_max, \n",
    "                      series_var=[\"x\", \"h\"], update_value={\"z\": \"z_prev\"})\n",
    "loss = _loss.expectation(rnn).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmm = Model(loss, distributions=[rnn, encoder, decoder, prior], \n",
    "            optimizer=optim.RMSprop, optimizer_params={\"lr\": 5e-4}, clip_grad_value=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributions (for training): \n",
      "  p(h|x), p(z|h,z_{prev}), p(x|z), p(z|z_{prev}) \n",
      "Loss function: \n",
      "  mean \\left(\\mathbb{E}_{p(h|x)} \\left[\\sum_{t=1}^{28} \\left(D_{KL} \\left[p(z|h,z_{prev})||p(z|z_{prev}) \\right] - \\mathbb{E}_{p(z|h,z_{prev})} \\left[\\log p(x|z) \\right]\\right) \\right] \\right) \n",
      "Optimizer: \n",
      "  RMSprop (\n",
      "  Parameter Group 0\n",
      "      alpha: 0.99\n",
      "      centered: False\n",
      "      eps: 1e-08\n",
      "      lr: 0.0005\n",
      "      momentum: 0\n",
      "      weight_decay: 0\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$mean \\left(\\mathbb{E}_{p(h|x)} \\left[\\sum_{t=1}^{28} \\left(D_{KL} \\left[p(z|h,z_{prev})||p(z|z_{prev}) \\right] - \\mathbb{E}_{p(z|h,z_{prev})} \\left[\\log p(x|z) \\right]\\right) \\right] \\right)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dmm)\n",
    "print_latex(dmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loop(epoch, loader, model, device, train_mode=False):\n",
    "    mean_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(tqdm(loader)):\n",
    "        data = data.to(device)\n",
    "        batch_size = data.size()[0]\n",
    "        x = data.transpose(0, 1)\n",
    "        z_prev = torch.zeros(batch_size, z_dim).to(device)\n",
    "        if train_mode:\n",
    "            mean_loss += model.train({'x': x, 'z_prev': z_prev}).item() * batch_size\n",
    "        else:\n",
    "            mean_loss += model.test({'x': x, 'z_prev': z_prev}).item() * batch_size\n",
    "    mean_loss /= len(loader.dataset)\n",
    "    if train_mode:\n",
    "        print('Epoch: {} Train loss: {:.4f}'.format(epoch, mean_loss))\n",
    "    else:\n",
    "        print('Test loss: {:.4f}'.format(mean_loss))\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_from_latent(batch_size):\n",
    "    x = []\n",
    "    z_prev = torch.zeros(batch_size, z_dim).to(device)\n",
    "    for step in range(t_max):\n",
    "        samples = generate_from_prior.sample({'z_prev': z_prev})\n",
    "        x_t = decoder.sample_mean({\"z\": samples[\"z\"]})\n",
    "        z_prev = samples[\"z\"]\n",
    "        x.append(x_t[None, :])\n",
    "    x = torch.cat(x, dim=0).transpose(0, 1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def reconstruct(data):\n",
    "    xs = []\n",
    "    data = data.to(device)\n",
    "    x = data.transpose(0, 1)\n",
    "    batch_size = data.size()[0]\n",
    "    z_prev = torch.zeros(batch_size, z_dim).to(device)\n",
    "    for t in range(t_max):\n",
    "        h_t = rnn.sample_mean({'x': x})[t]\n",
    "        z_t = encoder.sample_mean({'h': h_t, 'z_prev': z_prev})\n",
    "        dec_x = decoder.sample_mean({'z': z_t})\n",
    "        z_prev = z_t\n",
    "        xs.append(dec_x[None, :])\n",
    "    reconst_img = torch.cat(xs, dim=0).transpose(0, 1)\n",
    "    return reconst_img\n",
    "\n",
    "\n",
    "def sample_after_n_steps(num_step, data):\n",
    "    xs = []\n",
    "    data = data.to(device)\n",
    "    x = data.transpose(0, 1)\n",
    "    batch_size = data.size()[0]\n",
    "    z_prev = torch.zeros(batch_size, z_dim).to(device)\n",
    "    for t in range(t_max):\n",
    "        if t+1 <  num_step:\n",
    "            h_t = rnn.sample_mean({'x': x})[t]\n",
    "            z_t = encoder.sample_mean({'h': h_t, 'z_prev': z_prev})\n",
    "            dec_x = decoder.sample_mean({'z': z_t})\n",
    "            z_prev = z_t\n",
    "            xs.append(dec_x[None, :])\n",
    "        else:\n",
    "            z_t = prior.sample_mean({'z_prev':z_prev})\n",
    "            dec_x = decoder.sample_mean({'z': z_t})\n",
    "            z_prev = z_t\n",
    "            xs.append(dec_x[None, :])\n",
    "    sample_img = torch.cat(xs, dim=0).transpose(0, 1)\n",
    "    return sample_img\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:19<00:00,  5.86it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 210.5909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:06<00:00, 12.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 199.9833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:20<00:00,  5.79it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Train loss: 194.6889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:06<00:00, 13.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 188.0311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:20<00:00,  5.81it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Train loss: 185.8990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:06<00:00, 12.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 182.3530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:20<00:00,  5.80it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Train loss: 181.1654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:06<00:00, 13.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 179.2692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [01:20<00:00,  5.84it/s]\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Train loss: 179.0450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:05<00:00, 13.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 178.2976\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "_x, _ = iter(test_loader).next()\n",
    "_x = _x.to(device)\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = data_loop(epoch, train_loader, dmm, device, train_mode=True)\n",
    "    test_loss = data_loop(epoch, test_loader, dmm, device)\n",
    "\n",
    "    writer.add_scalar('train_loss', train_loss, epoch)\n",
    "    writer.add_scalar('test_loss', test_loss, epoch)\n",
    "\n",
    "    sample = plot_image_from_latent(batch_size)[:, None]\n",
    "    writer.add_images('Image_from_latent', sample, epoch)\n",
    "    \n",
    "    reconst_img = reconstruct(_x)[:, None]\n",
    "    writer.add_images('reconstructed_img', reconst_img, epoch)\n",
    "    \n",
    "    n_step_after_sample = sample_after_n_steps(14, _x)[:, None]\n",
    "    writer.add_images('n_step', n_step_after_sample, epoch)\n",
    "    \n",
    "    writer.add_images('orignal', _x[:, None], epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
