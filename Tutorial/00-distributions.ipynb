{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixyzの確率分布の記述方法\n",
    "\n",
    "ここではまず，Pixyzにおける確率モデルの実装方法について説明します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x112fa3670>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pixyz.utils import print_latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 シンプルな確率分布の設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ガウス分布を作るためには，`Normal`をインポートして，平均（loc）と標準偏差（scale）を定義します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixyz.distributions import Normal\n",
    "\n",
    "x_dim = 50\n",
    "p1 = Normal(loc=torch.tensor(0.), scale=torch.tensor(1.), var=['x'], features_shape=[x_dim], name='p_{1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なお``var``には，変数の名前を設定します．ここでは`\"x\"`を設定しています．\n",
    "\n",
    "また，dimでは次元数を指定します．ここではdimが50となっていますから，50次元のサンプルを生成する形になります．\n",
    "\n",
    "上記で定義したp1の情報は次のようにみることができます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal\n",
      "p_{1}(x)\n"
     ]
    }
   ],
   "source": [
    "print(p1.distribution_name) \n",
    "print(p1.prob_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distribution_nameでは，確率分布の名前を確認できます．\n",
    "\n",
    "prob_textでは，確率分布の形をテキストで出力できます．ここでテキストに書かれている確率変数は，上記のvarで指定したものです.\n",
    "\n",
    "また，p1を丸ごとprintすると，以下のように表示されます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p_{1}(x)\n",
      "Network architecture:\n",
      "  Normal(\n",
      "    name=p_{1}, distribution_name=Normal,\n",
      "    var=['x'], cond_var=[], input_var=[], features_shape=torch.Size([50])\n",
      "    (loc): torch.Size([1, 50])\n",
      "    (scale): torch.Size([1, 50])\n",
      "  )\n"
     ]
    }
   ],
   "source": [
    "print(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$p_{1}(x)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_latex(p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に，定義した分布からサンプリングしてみましょう． サンプリングは，`sample()`によって実行します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091,\n",
      "         -0.7121,  0.3037, -0.7773, -0.2515, -0.2223,  1.6871,  0.2284,  0.4676,\n",
      "         -0.6970, -1.1608,  0.6995,  0.1991,  0.8657,  0.2444, -0.6629,  0.8073,\n",
      "          1.1017, -0.1759, -2.2456, -1.4465,  0.0612, -0.6177, -0.7981, -0.1316,\n",
      "          1.8793, -0.0721,  0.0663, -0.4370,  0.7626,  0.4415,  1.1651,  2.0154,\n",
      "          0.2152, -0.5242, -0.1860, -0.6446,  1.5392, -0.8696, -3.3312, -0.7479,\n",
      "          1.1173,  0.2981]])}\n",
      "tensor([[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091,\n",
      "         -0.7121,  0.3037, -0.7773, -0.2515, -0.2223,  1.6871,  0.2284,  0.4676,\n",
      "         -0.6970, -1.1608,  0.6995,  0.1991,  0.8657,  0.2444, -0.6629,  0.8073,\n",
      "          1.1017, -0.1759, -2.2456, -1.4465,  0.0612, -0.6177, -0.7981, -0.1316,\n",
      "          1.8793, -0.0721,  0.0663, -0.4370,  0.7626,  0.4415,  1.1651,  2.0154,\n",
      "          0.2152, -0.5242, -0.1860, -0.6446,  1.5392, -0.8696, -3.3312, -0.7479,\n",
      "          1.1173,  0.2981]])\n"
     ]
    }
   ],
   "source": [
    "samples = p1.sample()\n",
    "print(samples)\n",
    "print(samples[\"x\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出力はdict形式になっています．\n",
    "\n",
    "サンプリング結果を確認したい変数について指定することで，中身を確認できます（ただし，この例では変数は\"x\"のみです）．\n",
    "\n",
    "なお，サンプリング結果は，PyTorchのtensor形式になっています．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いて，尤度の計算方法を説明します．\n",
    "例えば，次のサンプルがあったとしましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_sample1 = torch.Tensor([[-0.3030, -1.7618,  0.6348, -0.8044, -1.0371, -1.0669, -0.2085,\n",
    "         -0.2155,  2.2952,  0.6749,  1.7133, -1.7943, -1.5208,  0.9196,\n",
    "         -0.5484, -0.3472,  0.4730, -0.4286,  0.5514, -1.5474,  0.7575,\n",
    "         -0.4068, -0.1277,  0.2804,  1.7460,  1.8550, -0.7064,  2.5571,\n",
    "          0.7705, -1.0739, -0.2015, -0.5603, -0.6240, -0.9773, -0.1637,\n",
    "         -0.3582, -0.0594, -2.4919,  0.2423,  0.2883, -0.1095,  0.3126,\n",
    "         -0.3417,  0.9473,  0.6223, -0.4481, -0.2856,  0.3880, -1.1435,\n",
    "         -0.6512]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このサンプルにおける，p1の対数尤度を計算します．\n",
    "\n",
    "これは，log_likelihoodによって簡単に計算できます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\log p_{1}(x)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_like = p1.log_prob()\n",
    "print_latex(log_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-72.5003])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_like.eval({'x': _sample1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なお，log_likelihoodの引数はdictで与えることに注意してください． これは，どの確率変数かを指定するためです（この例では，xしかありませんが）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 以下何をしたいのか書く\n",
    "# 確率の積ができることを示したい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_dim = 50\n",
    "_p1 = Normal(loc='mu', scale=torch.tensor(1.), var=['x'], cond_var=['mu'], features_shape=[x_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p(x|\\mu)\n",
      "Network architecture:\n",
      "  Normal(\n",
      "    name=p, distribution_name=Normal,\n",
      "    var=['x'], cond_var=['mu'], input_var=['mu'], features_shape=torch.Size([50])\n",
      "    (scale): torch.Size([1, 50])\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$p(x|\\mu)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(_p1)\n",
    "print_latex(_p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mu': 0,\n",
       " 'x': tensor([[-0.5962, -1.0055, -0.2106, -0.0075,  1.6734,  0.0103,  0.9837,  0.8793,\n",
       "          -0.9962, -0.8313, -0.4610, -0.5601,  0.3956, -0.9823,  1.3264,  0.8547,\n",
       "          -0.6540,  0.7317, -1.4344, -0.5008,  0.1716, -0.1600, -0.5047, -1.4746,\n",
       "          -1.0412,  0.7323, -1.0483, -0.4709,  0.2911,  1.9907, -0.9247, -0.9301,\n",
       "           0.8165, -0.9135,  0.2053,  0.3051,  0.5357, -0.4312,  0.1573,  1.2540,\n",
       "           1.3275, -0.4954, -1.9804,  1.7986,  0.1018,  0.3400, -0.6447, -0.2870,\n",
       "           3.3212, -0.4021]])}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_p1.sample({\"mu\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_p2 = Normal(loc=torch.tensor(0.), scale=torch.tensor(1.), var=['mu'], features_shape=[x_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p(\\mu)\n",
      "Network architecture:\n",
      "  Normal(\n",
      "    name=p, distribution_name=Normal,\n",
      "    var=['mu'], cond_var=[], input_var=[], features_shape=torch.Size([50])\n",
      "    (loc): torch.Size([1, 50])\n",
      "    (scale): torch.Size([1, 50])\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$p(\\mu)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(_p2)\n",
    "print_latex(_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 2_3と被ってない？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pixyzでは分布の積は，掛け算で表すことができます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p(x,\\mu) = p(x|\\mu)p(\\mu)\n",
      "Network architecture:\n",
      "  Normal(\n",
      "    name=p, distribution_name=Normal,\n",
      "    var=['mu'], cond_var=[], input_var=[], features_shape=torch.Size([50])\n",
      "    (loc): torch.Size([1, 50])\n",
      "    (scale): torch.Size([1, 50])\n",
      "  )\n",
      "  Normal(\n",
      "    name=p, distribution_name=Normal,\n",
      "    var=['x'], cond_var=['mu'], input_var=['mu'], features_shape=torch.Size([50])\n",
      "    (scale): torch.Size([1, 50])\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$p(x,\\mu) = p(x|\\mu)p(\\mu)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_p3 = _p1 * _p2\n",
    "print(_p3) \n",
    "print_latex(_p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sampleのつながりについてより書きたい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mu': tensor([[-0.3030, -1.7618,  0.6348, -0.8044, -1.0371, -1.0669, -0.2085, -0.2155,\n",
       "           2.2952,  0.6749,  1.7133, -1.7943, -1.5208,  0.9196, -0.5484, -0.3472,\n",
       "           0.4730, -0.4286,  0.5514, -1.5474,  0.7575, -0.4068, -0.1277,  0.2804,\n",
       "           1.7460,  1.8550, -0.7064,  2.5571,  0.7705, -1.0739, -0.2015, -0.5603,\n",
       "          -0.6240, -0.9773, -0.1637, -0.3582, -0.0594, -2.4919,  0.2423,  0.2883,\n",
       "          -0.1095,  0.3126, -0.3417,  0.9473,  0.6223, -0.4481, -0.2856,  0.3880,\n",
       "          -1.1435, -0.6512]]),\n",
       " 'x': tensor([[-3.2197, -2.3292, -0.5643, -0.8518, -3.0401, -1.5583,  1.5001,  0.5758,\n",
       "           3.0235,  0.7320,  1.0042, -2.3205, -2.6250,  0.2201,  0.0096, -1.2531,\n",
       "           2.3094,  0.8959,  0.4809, -1.2004,  0.1038,  1.1519,  0.2723,  2.7226,\n",
       "           3.2031,  3.6259, -2.7237,  2.9806,  1.3436, -2.8702, -0.5077, -0.9805,\n",
       "          -0.2331, -0.5901, -0.0193,  0.4189, -2.3976, -3.3211,  1.4084,  1.7670,\n",
       "           0.1581,  1.0687, -0.9290, -1.1146,  1.0528, -0.1105, -0.6294, -0.2292,\n",
       "           0.1095, -0.7026]])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_p3.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 深層ニューラルネットワークと組み合わせた確率分布の設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に， 確率分布のパラメータを深層ニューラルネットワークで定義します．\n",
    "\n",
    "例えば，ガウス分布の平均$\\mu$と分散$\\sigma^2$は， パラメータ$\\theta$を持つ深層ニューラルネットワークによって，$\\mu=f(x;\\theta)$および$\\sigma^2=g(x;\\theta)$と定義できます．\n",
    "\n",
    "したがって，ガウス分布は${\\cal N}(\\mu=f(x;\\theta),\\sigma^2=g(x;\\theta))$となります．\n",
    "\n",
    "Pixyzでは，次のようなクラスを記述することで，これを実現できます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a_dim = 20\n",
    "\n",
    "class P2(Normal):\n",
    "    def __init__(self):\n",
    "        super(P2, self).__init__(cond_var=['x'], var=['a'])\n",
    "        \n",
    "        self.fc1 = nn.Linear(x_dim, 10)\n",
    "        self.fc21 = nn.Linear(10, a_dim)\n",
    "        self.fc22 = nn.Linear(10, a_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return {'loc': self.fc21(h1), 'scale': F.softplus(self.fc22(h1))}\n",
    "p2 = P2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loc, scaleとは？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず， ガウス分布クラスを継承することで，ガウス分布のパラメータを深層ニューラルネットワークで定義することを明示します．\n",
    "\n",
    "次に，コンストラクタで，利用するニューラルネットワークを記述します．これは，通常のPyTorchと同じです．\n",
    "\n",
    "唯一異なる点は，superの引数にvarとcond_varの名前を指定している点です．\n",
    "\n",
    "varは先程見たように，出力する変数の名前を指定します．一方，cond_varではニューラルネットワークの入力変数の名前を指定します．これは，ここで定義する分布において，条件付けられる変数とみなすことができます．\n",
    "\n",
    "forwardについても，通常のPyTorchと同じです．ただし，注意点が2つあります．\n",
    "\n",
    "* 引数の名前と数は，cond_varで設定したものと同じにしてください． 例えば，cond_var=[\"x\", \"y\"]とした場合は，forward(self, x, y)としてください．ただし，引数の順番は変えても構いません．\n",
    "* 戻り値は，それぞれの確率分布のパラメータになります．上記の例ではガウス分布なので，平均と分散を指定しています．\n",
    "\n",
    "そして最後に，定義した確率分布クラスのインスタンスを作成します．\n",
    "\n",
    "次に，先程の例と同様，確率分布の情報を見てみましょう."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p(a|x)\n",
      "Network architecture:\n",
      "  P2(\n",
      "    name=p, distribution_name=Normal,\n",
      "    var=['a'], cond_var=['x'], input_var=['x'], features_shape=torch.Size([])\n",
      "    (fc1): Linear(in_features=50, out_features=10, bias=True)\n",
      "    (fc21): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (fc22): Linear(in_features=10, out_features=20, bias=True)\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$p(a|x)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(p2)\n",
    "print_latex(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p2の分布は，xで条件付けた形になっています．これらの表記は，superの引数で設定したとおりになっています．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に，先程の例のように，サンプリングしてみましょう．\n",
    "\n",
    "注意しなければならないのは，先ほどと異なり，条件づけた変数xがあるということです．\n",
    "\n",
    "先程おいた_sample1をxとしてサンプリングしましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## sampleという名前いけてないな，パッとなんのサンプルかわからない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([[-0.3030, -1.7618,  0.6348, -0.8044, -1.0371, -1.0669, -0.2085, -0.2155,\n",
      "          2.2952,  0.6749,  1.7133, -1.7943, -1.5208,  0.9196, -0.5484, -0.3472,\n",
      "          0.4730, -0.4286,  0.5514, -1.5474,  0.7575, -0.4068, -0.1277,  0.2804,\n",
      "          1.7460,  1.8550, -0.7064,  2.5571,  0.7705, -1.0739, -0.2015, -0.5603,\n",
      "         -0.6240, -0.9773, -0.1637, -0.3582, -0.0594, -2.4919,  0.2423,  0.2883,\n",
      "         -0.1095,  0.3126, -0.3417,  0.9473,  0.6223, -0.4481, -0.2856,  0.3880,\n",
      "         -1.1435, -0.6512]]), 'a': tensor([[-1.0360e+00,  9.8996e-02,  2.5841e-01, -5.8859e-01,  5.1662e-04,\n",
      "          5.9433e-01, -2.5836e-01,  4.7657e-01, -4.7303e-02,  6.7520e-01,\n",
      "          2.3160e-01,  2.7852e-01, -1.4851e+00,  3.7574e-01,  1.7171e-01,\n",
      "          4.6762e-01, -6.4956e-01,  1.0501e-01,  4.8974e-01,  8.8420e-01]])}\n",
      "tensor([[-1.0360e+00,  9.8996e-02,  2.5841e-01, -5.8859e-01,  5.1662e-04,\n",
      "          5.9433e-01, -2.5836e-01,  4.7657e-01, -4.7303e-02,  6.7520e-01,\n",
      "          2.3160e-01,  2.7852e-01, -1.4851e+00,  3.7574e-01,  1.7171e-01,\n",
      "          4.6762e-01, -6.4956e-01,  1.0501e-01,  4.8974e-01,  8.8420e-01]])\n",
      "tensor([[-0.3030, -1.7618,  0.6348, -0.8044, -1.0371, -1.0669, -0.2085, -0.2155,\n",
      "          2.2952,  0.6749,  1.7133, -1.7943, -1.5208,  0.9196, -0.5484, -0.3472,\n",
      "          0.4730, -0.4286,  0.5514, -1.5474,  0.7575, -0.4068, -0.1277,  0.2804,\n",
      "          1.7460,  1.8550, -0.7064,  2.5571,  0.7705, -1.0739, -0.2015, -0.5603,\n",
      "         -0.6240, -0.9773, -0.1637, -0.3582, -0.0594, -2.4919,  0.2423,  0.2883,\n",
      "         -0.1095,  0.3126, -0.3417,  0.9473,  0.6223, -0.4481, -0.2856,  0.3880,\n",
      "         -1.1435, -0.6512]])\n"
     ]
    }
   ],
   "source": [
    "samples = p2.sample({'x': _sample1})\n",
    "print(samples)\n",
    "print(samples['a'])\n",
    "print(samples['x'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出力には，aとxの２つのサンプルがあります．\n",
    "\n",
    "aが今回計算したサンプルで，xについては，引数として与えたサンプルがそのまま入っています．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に，尤度計算をします．\n",
    "\n",
    "尤度計算では， 全ての変数のデータを与える必要があります．\n",
    "\n",
    "aについては，上記でサンプルした値を入れて計算しましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-15.8274], grad_fn=<SumBackward2>)\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$\\log p(a|x)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sample2 = samples['a']\n",
    "log_like = p2.log_prob()\n",
    "print(log_like.eval({'x': _sample1, 'a': _sample2}))\n",
    "print_latex(log_like)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なお，これはもっと簡単に書くことができます．\n",
    "\n",
    "上記のサンプリングで全ての変数とその値がdict形式で出力されたので，それをそのままlog_likelihoodの引数とすればいいのです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-15.8274], grad_fn=<SumBackward2>)\n"
     ]
    }
   ],
   "source": [
    "print(log_like.eval(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように記述できる利点は， **変数の数が増えても同じ書き方で尤度計算を実行できる**ことです．\n",
    "\n",
    "サンプリング->尤度計算という処理は，深層生成モデルでは数多く登場します． このように記述できることで，どのような確率分布の形であっても容易に尤度を計算できるようになるのです．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 確率分布の積の設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初に書いたとおり，Pixyzの特徴の1つは，確率分布の積を簡単に記述できることです．\n",
    "\n",
    "ここで，これまで設定した確率分布を再度確認しましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p_{1}(x)\n",
      "Network architecture:\n",
      "  Normal(\n",
      "    name=p_{1}, distribution_name=Normal,\n",
      "    var=['x'], cond_var=[], input_var=[], features_shape=torch.Size([50])\n",
      "    (loc): torch.Size([1, 50])\n",
      "    (scale): torch.Size([1, 50])\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$p_{1}(x)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(p1) \n",
    "print_latex(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p(a|x)\n",
      "Network architecture:\n",
      "  P2(\n",
      "    name=p, distribution_name=Normal,\n",
      "    var=['a'], cond_var=['x'], input_var=['x'], features_shape=torch.Size([])\n",
      "    (fc1): Linear(in_features=50, out_features=10, bias=True)\n",
      "    (fc21): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (fc22): Linear(in_features=10, out_features=20, bias=True)\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$p(a|x)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(p2)\n",
    "print_latex(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数式的には，これらを掛け合わせることで同時分布を定義できます． \n",
    "\n",
    "$p(x,a) = p(a|x)p(x)$\n",
    "\n",
    "では，Pixyzではこれをどのように記述するのでしょうか．\n",
    "\n",
    "実は，それぞれの確率分布のインスタンスを**文字通り掛け合わせるだけ**でいいのです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p3 = p1 * p2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p3ではどのような確率分布が定義されているか確認しましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p(a,x) = p(a|x)p_{1}(x)\n",
      "Network architecture:\n",
      "  Normal(\n",
      "    name=p_{1}, distribution_name=Normal,\n",
      "    var=['x'], cond_var=[], input_var=[], features_shape=torch.Size([50])\n",
      "    (loc): torch.Size([1, 50])\n",
      "    (scale): torch.Size([1, 50])\n",
      "  )\n",
      "  P2(\n",
      "    name=p, distribution_name=Normal,\n",
      "    var=['a'], cond_var=['x'], input_var=['x'], features_shape=torch.Size([])\n",
      "    (fc1): Linear(in_features=50, out_features=10, bias=True)\n",
      "    (fc21): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (fc22): Linear(in_features=10, out_features=20, bias=True)\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$p(a,x) = p(a|x)p_{1}(x)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(p3)\n",
    "print_latex(p3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確かに，同時分布が定義されていることがわかります．\n",
    "\n",
    "このインスタンスp3からも，これまでと同様にサンプリングや尤度計算ができます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-82.8117], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$\\log p(a,x)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = p3.sample()\n",
    "log_like = p3.log_prob().eval(samples)\n",
    "print(log_like)\n",
    "print_latex(p3.log_prob())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## どういうこと？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この例をみてわかるように，サンプリングや尤度計算において，もはやp3を構成する各分布の形を気にする必要はありません．\n",
    "\n",
    "このような記述方法は，Pythonにおける確率モデリングライブラリでは**Pixyzが初めて採用しました**．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これは，確率分布が増えても同じです． 例えば，次の確率分布を定義しましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p(y|a,x)\n",
      "Network architecture:\n",
      "  P4(\n",
      "    name=p, distribution_name=Normal,\n",
      "    var=['y'], cond_var=['a', 'x'], input_var=['a', 'x'], features_shape=torch.Size([])\n",
      "    (fc1): Linear(in_features=50, out_features=10, bias=True)\n",
      "    (fc2): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (fc21): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (fc22): Linear(in_features=20, out_features=20, bias=True)\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$p(y|a,x)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class P4(Normal):\n",
    "    def __init__(self):\n",
    "        super(P4, self).__init__(cond_var=[\"a\", \"x\"], var=[\"y\"])\n",
    "\n",
    "        self.fc1 = nn.Linear(x_dim, 10)\n",
    "        self.fc2 = nn.Linear(a_dim, 10)\n",
    "        self.fc21 = nn.Linear(10+10, 20)\n",
    "        self.fc22 = nn.Linear(10+10, 20)\n",
    "\n",
    "    def forward(self, a, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(a))\n",
    "        h12 = torch.cat([h1, h2], 1)\n",
    "        return {\"loc\":self.fc21(h12), \"scale\":F.softplus(self.fc22(h12))}\n",
    "    \n",
    "p4 = P4()\n",
    "\n",
    "print(p4)\n",
    "print_latex(p4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p4は，aとxで条件付けられた確率分布です．他に$p(a|x)$，$p(x)$をつかって，同時分布は以下のように書けます．\n",
    "\n",
    "$p(y,a,x)=p(y|a,x)p(a|x)p(x)$\n",
    "\n",
    "同時分布が3つの確率分布の積になっていますが，これも上記と同様に書けます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p(y,a,x) = p(y|a,x)p(a|x)p_{1}(x)\n",
      "Network architecture:\n",
      "  Normal(\n",
      "    name=p_{1}, distribution_name=Normal,\n",
      "    var=['x'], cond_var=[], input_var=[], features_shape=torch.Size([50])\n",
      "    (loc): torch.Size([1, 50])\n",
      "    (scale): torch.Size([1, 50])\n",
      "  )\n",
      "  P2(\n",
      "    name=p, distribution_name=Normal,\n",
      "    var=['a'], cond_var=['x'], input_var=['x'], features_shape=torch.Size([])\n",
      "    (fc1): Linear(in_features=50, out_features=10, bias=True)\n",
      "    (fc21): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (fc22): Linear(in_features=10, out_features=20, bias=True)\n",
      "  )\n",
      "  P4(\n",
      "    name=p, distribution_name=Normal,\n",
      "    var=['y'], cond_var=['a', 'x'], input_var=['a', 'x'], features_shape=torch.Size([])\n",
      "    (fc1): Linear(in_features=50, out_features=10, bias=True)\n",
      "    (fc2): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (fc21): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (fc22): Linear(in_features=20, out_features=20, bias=True)\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$p(y,a,x) = p(y|a,x)p(a|x)p_{1}(x)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p5 = p4 * p2 * p1\n",
    "print(p5)\n",
    "print_latex(p5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distributionをみると，p5がどのような分布から構成されているかを表示します． サンプリングや尤度計算も全く同じようにできます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-109.5795], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$\\log p(y,a,x)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = p5.sample()\n",
    "log_like = p5.log_prob().eval(samples)\n",
    "print(log_like)\n",
    "print_latex(p5.log_prob())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように，分布や深層ニューラルネットワークの形を気にせずに，同じ記述方法でサンプリングや尤度計算ができます．\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
