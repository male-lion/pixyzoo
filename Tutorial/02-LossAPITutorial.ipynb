{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深層生成モデルではモデルの設計=目的関数の定義\n",
    "- 深層生成モデルでは，いずれのモデルも最適化するための目的関数を明示的に設定する\n",
    "    - 自己回帰モデル・フローベースモデル: Kullback-Leiblerダイバージェンス(対数尤度)\n",
    "    - VAE: 周辺対数尤度の下界\n",
    "    - GAN: Jensen-Shannonダイバージェンス(ただし目的関数自身の更新も必要(=敵対的学習))\n",
    "- 推論，確率変数の表現の正則化なども，全て目的関数として追加する\n",
    "<img src='tutorial_figs/vae_loss.png'>\n",
    "   \n",
    "    - 深層生成モデルではモデルの設計=目的関数の定義\n",
    "    - 従来の生成モデルとは異なり，サンプリングによる推論等は行わない\n",
    "- 確率分布を受け取って目的関数を定義できる枠組みが必要\n",
    "    - LossAPI  \n",
    "<img src='tutorial_figs/PixyzAPI.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 確率分布を受け取って目的関数を定義する\n",
    "- Loss API document: https://pixyz.readthedocs.io/en/latest/losses.html#\n",
    "\n",
    "ここではDistribution APIで定義した確率分布を受け取り目的関数を定義するまでの流れを確認する  \n",
    "目的関数を定義する際には以下の項目が必要となる\n",
    "1. 尤度計算をする\n",
    "1. 確率分布の距離を計算する\n",
    "1. 期待値を計算する\n",
    "1. データ分布を考慮した計算(mean, sum)  \n",
    "\n",
    "VAEのLossではそれぞれの項目は以下のように対応\n",
    "<img src='tutorial_figs/vae_loss_API.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10dc50650>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pixyz module\n",
    "from pixyz.distributions import Normal\n",
    "from pixyz.utils import print_latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尤度計算を行う\n",
    "ある観測値$x_1$, ...., $x_N$が得られた際，xが従うと仮定した確率分布pの尤もらしさを計算します  \n",
    "ここではxは平均0, 分散1の正規分布に従うのではないかと仮定します  \n",
    "$p(x) = \\cal N(\\mu=0, \\sigma^2=1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p(x)\n",
      "Network architecture:\n",
      "  Normal(\n",
      "    name=p, distribution_name=Normal,\n",
      "    var=['x'], cond_var=[], input_var=[], features_shape=torch.Size([5])\n",
      "    (loc): torch.Size([1, 5])\n",
      "    (scale): torch.Size([1, 5])\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$p(x)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 確率分布pを定義\n",
    "x_dim = 5\n",
    "p_nor_x = Normal(var=['x'], loc=torch.tensor(0.), scale=torch.tensor(1.), features_shape=[x_dim])\n",
    "print(p_nor_x)\n",
    "print_latex(p_nor_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 5])\n"
     ]
    }
   ],
   "source": [
    "# xを観測\n",
    "observed_x_num = 100\n",
    "observed_x = torch.randn(observed_x_num, x_dim)\n",
    "print(observed_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "対数尤度は以下のように計算されます  \n",
    "$L=\\sum_{i=1}^{100} \\log p\\left(x_{i}\\right)$  \n",
    "PixyzではLogProbを使用することで簡単に計算できます  \n",
    "LogProbの引数にPixyz Distributionで定義した確率分布を格納し  \n",
    "観測値をLogProb.eval()で渡すことで計算が行われます  \n",
    "Pixyz document: https://docs.pixyz.io/en/latest/losses.html#probability-density-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\log p(x)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pixyz.losses import LogProb\n",
    "# LogProbの引数にPixyz Distributionで定義した確率分布を格納\n",
    "log_likelihood_x = LogProb(p_nor_x)\n",
    "print_latex(log_likelihood_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -7.5539,  -6.8545,  -6.4024,  -5.8851,  -6.1517,  -8.3702,  -6.7028,\n",
      "         -5.0395,  -7.4346,  -7.1497,  -5.7594,  -7.3006, -11.9857,  -5.8238,\n",
      "         -6.7561,  -5.7640,  -6.2382,  -4.9060,  -6.1076,  -8.2535,  -7.8250,\n",
      "         -7.1956,  -7.6949,  -5.2324, -11.5860,  -8.1068,  -7.1763,  -8.3332,\n",
      "        -11.4631,  -6.6297,  -6.1200, -12.2358,  -5.3402,  -7.1465,  -7.5106,\n",
      "         -7.0829,  -6.6300,  -6.1832,  -7.2049, -10.8676,  -6.8674,  -5.8339,\n",
      "         -9.1939,  -7.5965,  -8.7743,  -7.3492,  -5.2578, -10.3097,  -6.5646,\n",
      "         -4.8807,  -5.9738,  -6.2394, -10.3945,  -9.1760,  -9.2957,  -5.5627,\n",
      "         -7.1047,  -6.4066,  -6.8100,  -6.0878,  -6.8835,  -7.9132,  -5.0738,\n",
      "         -8.8378,  -6.2286,  -5.8401,  -5.9691,  -5.6857,  -7.6903,  -6.4982,\n",
      "         -7.1259,  -8.7953, -10.5572,  -5.9161,  -7.0649,  -6.1292,  -6.0871,\n",
      "         -7.2513,  -7.2517,  -7.1378,  -6.4228,  -5.5728,  -5.6155,  -5.1962,\n",
      "         -8.3940,  -7.8178,  -9.8129,  -6.1119,  -5.0492,  -8.9898,  -6.9675,\n",
      "         -8.0218, -13.9816,  -6.8575,  -5.1304,  -5.5069,  -5.0561,  -5.1264,\n",
      "         -4.8489,  -5.4876])\n",
      "observed_x_num:  100\n"
     ]
    }
   ],
   "source": [
    "# 観測値それぞれに対しての尤度が計算される\n",
    "print(log_likelihood_x.eval({'x': observed_x}))\n",
    "# observed_x_num = 100\n",
    "print('observed_x_num: ', len(log_likelihood_x.eval({'x': observed_x})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log_likelihood_x.eval({'x': observed_x})には  \n",
    "$\\log p(x_{1})$, $\\log p(x_{2})$, ...., $\\log p(x_{100})$  \n",
    "の計算結果が格納されている  \n",
    "log_likelihood_x.eval({'x': observed_x})[i] = $\\log p(x_{i})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に各要素を合計し\n",
    "$L=\\sum_{i=1}^{100} \\log p\\left(x_{i}\\right)$を計算する  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "対数尤度の計算結果: tensor(-715.5875)\n"
     ]
    }
   ],
   "source": [
    "# 値を合計し対数尤度を計算する\n",
    "print('対数尤度の計算結果:', log_likelihood_x.eval({'x': observed_x}).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上のようにpixyz.lossesのLogProbを用いることで対数尤度が簡単に計算できることを確認しました  \n",
    "また，定義した確率分布からp.log_prob().eval()でも同様に計算が行えます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogProb()\n",
      "tensor(-715.5875)\n",
      ".log_prob()\n",
      "tensor(-715.5875)\n"
     ]
    }
   ],
   "source": [
    "print('LogProb()')\n",
    "print(LogProb(p_nor_x).eval({'x': observed_x}).sum())\n",
    "print('.log_prob()')\n",
    "print(p_nor_x.log_prob().eval({'x': observed_x}).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more Loss API related to probability density function:  \n",
    "https://docs.pixyz.io/en/latest/losses.html#probability-density-function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 確率分布の距離を計算する\n",
    "生成モデルの学習では真の分布(データ分布)$p_{data}(x)$と近いモデル分布(生成モデル)$p_{\\theta}(x)$を考え，適切な$\\theta$を求めるために分布間の距離を測ることがある \n",
    "\n",
    "VAE系ではKullback-Leiblerダイバージェンス, GAN系ではJensen-Shannonダイバージェンスといったように，確率分布間の距離を計算する  \n",
    "分布間距離の計算もPixyz Loss APIを用いれば簡単に行うことができる  \n",
    "Pixyz document:  \n",
    "https://docs.pixyz.io/en/latest/losses.html#statistical-distance  \n",
    "https://pixyz.readthedocs.io/en/latest/losses.html#adversarial-statistical-distance\n",
    "\n",
    "ここでは例として平均0, 分散1の正規分布pと平均5, 分散0.1の正規分布qとのKullback-Leiblerダイバージェンスを計算する  \n",
    "$p(x) = \\cal N(\\mu=0, \\sigma^2=1)$  \n",
    "$q(x) = \\cal N(\\mu=5, \\sigma^2=0.1)$  \n",
    "$KL(q(x) || p(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$p(x)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 確率分布の定義\n",
    "x_dim = 10\n",
    "# p \n",
    "p_nor_x = Normal(var=['x'], loc=torch.tensor(0.), scale=torch.tensor(1.), features_shape=[x_dim])\n",
    "print_latex(p_nor_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$q(x)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q\n",
    "q_nor_x = Normal(var=['x'], loc=torch.tensor(5.), scale=torch.tensor(0.1), features_shape=[x_dim], name='q')\n",
    "print_latex(q_nor_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kullback-Leiblerダイバージェンスを計算はpixyz.lossesのKullbackLeiblerを用いる  \n",
    "KullbackLeibler()の引数に距離を測りたい分布を格納し   \n",
    ".eval()で計算が行われる  \n",
    "Pixyz document: https://docs.pixyz.io/en/latest/losses.html#kullbackleibler  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$D_{KL} \\left[q(x)||p(x) \\right]$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pixyz.losses import KullbackLeibler\n",
    "\n",
    "kl_q_p = KullbackLeibler(q_nor_x, p_nor_x)\n",
    "print_latex(kl_q_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([143.0759])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .eval で計算を行う\n",
    "kl_q_p.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more Loss API related to statistical distance: \n",
    "https://docs.pixyz.io/en/latest/losses.html#statistical-distance  \n",
    "https://docs.pixyz.io/en/latest/losses.html#adversarial-statistical-distance  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 期待値を計算する\n",
    "生成モデルにおいて, 訓練データに存在しない特定の変数(VAE系では潜在変数)を仮定している場合, 尤度の最大化の計算をする際, そのままだと最大化できないため(Todo: なぜか，数式で説明を載せたい)周辺化する必要がある  \n",
    "特定の変数を周辺化する際に期待値計算は必要になる  \n",
    "期待値の計算もLoss APIを用いれば簡単に計算できる  \n",
    "Pixyz document:  \n",
    "https://docs.pixyz.io/en/latest/losses.html#expected-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは例として  \n",
    "$q(z|x) = \\cal N(\\mu=x, \\sigma^2=1)$  \n",
    "$p(x|z) = \\cal N(\\mu=z, \\sigma^2=1)$  \n",
    "といった二つの確率分布q(z|x), p(x|z)を考え  \n",
    "$\\mathbb{E}_{q(z|x)} \\left[\\log p(x|z) \\right]$を計算する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 確率分布の定義\n",
    "from pixyz.distributions import Normal\n",
    "\n",
    "q_nor_z__x = Normal(loc=\"x\", scale=torch.tensor(1.), var=[\"z\"], cond_var=[\"x\"],\n",
    "           features_shape=[10], name='q') # q(z|x)\n",
    "p_nor_x__z = Normal(loc=\"z\", scale=torch.tensor(1.), var=[\"x\"], cond_var=[\"z\"],\n",
    "                    features_shape=[10]) # p(x|z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\log p(x|z)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p(x|z)の対数尤度をとる\n",
    "from pixyz.losses import LogProb\n",
    "\n",
    "p_log_likelihood = LogProb(p_nor_x__z)\n",
    "print_latex(p_log_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "期待値の計算はpixyz.lossesのExpectationを用いる\n",
    "Expectation()の引数にはp, fがあり\n",
    "pに  \n",
    "fに  \n",
    "Todo: 説明  \n",
    ".eval()で計算が行われる  \n",
    "Pixyz document: https://docs.pixyz.io/en/latest/losses.html#expected-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\mathbb{E}_{q(z|x)} \\left[\\log p(x|z) \\right]$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pixyz.losses import Expectation as E\n",
    "\n",
    "E_q_logprob_p = E(q_nor_z__x, LogProb(p_nor_x__z))\n",
    "print_latex(E_q_logprob_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-10.7006, -11.9861])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_x = torch.randn(2, 10)\n",
    "E_q_logprob_p.eval({'x': sample_x}) # どういうこと？？どういう状況なのか？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details about Expectatoin API:  \n",
    "https://docs.pixyz.io/en/latest/losses.html#expected-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ分布を考慮した計算(mean, sum)\n",
    "本来ならxについて期待値をとる必要があるが，データ分布は実際に与えられないためbatch方向について平均や合計といった計算を行う  \n",
    "合計や平均といった計算もLoss APIでは簡単に行うことができる  \n",
    "ここではobserved_xを訓練データとして尤度計算を行いそのmeanを計算する\n",
    "$p(x) = \\cal N(\\mu=0, \\sigma^2=1)$  \n",
    "$\\frac{1}{N} \\sum_{i=1}^N\\left[\\log p\\left(x^{(i)}\\right)\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 5])\n"
     ]
    }
   ],
   "source": [
    "# xを観測\n",
    "observed_x_num = 100\n",
    "x_dim = 5\n",
    "observed_x = torch.randn(observed_x_num, x_dim)\n",
    "print(observed_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p(x)\n",
      "Network architecture:\n",
      "  Normal(\n",
      "    name=p, distribution_name=Normal,\n",
      "    var=['x'], cond_var=[], input_var=[], features_shape=torch.Size([5])\n",
      "    (loc): torch.Size([1, 5])\n",
      "    (scale): torch.Size([1, 5])\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$p(x)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 確率分布pを定義\n",
    "p_nor_x = Normal(var=['x'], loc=torch.tensor(0.), scale=torch.tensor(1.), features_shape=[x_dim])\n",
    "print(p_nor_x)\n",
    "print_latex(p_nor_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合計や平均といった計算はLoss.mean()やLoss.sum()とすることで容易に行える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$mean \\left(\\log p(x) \\right)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pixyz.losses import LogProb\n",
    "# meanを計算する\n",
    "mean_log_likelihood_x = LogProb(p_nor_x).mean() # .mean()\n",
    "print_latex(mean_log_likelihood_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-7.2453)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_log_likelihood_x.eval({'x': observed_x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lossの組み合わせ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高度なLossAPI(ELBO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### さらに高度なLossAPI(VAE, GAN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
