{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深層生成モデルの学習方法はモデルに依存しない\n",
    "### 目的関数と最適化アルゴリズムが独立に設定・学習できる枠組みが必要\n",
    "- 深層生成モデルでは，モデルに依存せずに勾配降下法で学習する\n",
    "    - 通常のDNNの学習と同様，様々なモデルに対して，任意の最適化アルゴリズムを用いる\n",
    "    - 従来の生成モデルのように，モデルに応じて決まった学習アルゴリズムが選択されることはない\n",
    "        - (どういう意図の列挙？): ギブスサンプリング，EMアルゴリズム，変分推論\n",
    "- なお，深層生成モデルはパラメータについて最尤推定(パラメータの分布は考えない)\n",
    "    - この点で，深層生成モデルはベイズ的ニューラルネットワークとは異なる  \n",
    "<img src='tutorial_figs/PixyzAPI.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目的関数と最適化アルゴリズムが独立に設定できる枠組み(Model API)\n",
    "- Model API document: https://docs.pixyz.io/en/v0.0.4/models.html  \n",
    "\n",
    "ここでは定義した確率分布と目的関数を受け取り，モデルの学習を行う流れを確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10c8196f0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "batch_size = 256\n",
    "seed = 1\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST datasetの準備\n",
    "root = 'data'\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Lambda(lambd=lambda x: x.view(-1))])\n",
    "kwargs = {'batch_size': batch_size, 'num_workers': 1, 'pin_memory': True}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(root=root, train=True, transform=transform, download=True),\n",
    "    shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(root=root, train=False, transform=transform),\n",
    "    shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 確率分布の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixyz.distributions import Normal, Bernoulli\n",
    "\n",
    "x_dim = 784\n",
    "z_dim = 64\n",
    "\n",
    "# inference model q(z|x)\n",
    "class Inference(Normal):\n",
    "    def __init__(self):\n",
    "        super(Inference, self).__init__(cond_var=[\"x\"], var=[\"z\"], name=\"q\")\n",
    "\n",
    "        self.fc1 = nn.Linear(x_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc31 = nn.Linear(512, z_dim)\n",
    "        self.fc32 = nn.Linear(512, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return {\"loc\": self.fc31(h), \"scale\": F.softplus(self.fc32(h))}\n",
    "\n",
    "    \n",
    "# generative model p(x|z)    \n",
    "class Generator(Bernoulli):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__(cond_var=[\"z\"], var=[\"x\"], name=\"p\")\n",
    "\n",
    "        self.fc1 = nn.Linear(z_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, x_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = F.relu(self.fc1(z))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return {\"probs\": torch.sigmoid(self.fc3(h))}\n",
    "    \n",
    "gen_ber_x__z = Generator().to(device)\n",
    "infer_nor_z__x = Inference().to(device)\n",
    "\n",
    "prior_nor_z = Normal(loc=torch.tensor(0.), scale=torch.tensor(1.),\n",
    "               var=[\"z\"], features_shape=[z_dim], name=\"p_{prior}\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lossの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$mean \\left(D_{KL} \\left[q(z|x)||p_{prior}(z) \\right] - \\mathbb{E}_{q(z|x)} \\left[\\log p(x|z) \\right] \\right)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lossの定義\n",
    "from pixyz.losses import LogProb\n",
    "from pixyz.losses import StochasticReconstructionLoss\n",
    "from pixyz.losses import Expectation as E\n",
    "from pixyz.losses import KullbackLeibler\n",
    "from pixyz.utils import print_latex\n",
    "\n",
    "# 対数尤度\n",
    "logprob_gen_x__z = LogProb(gen_ber_x__z)\n",
    "\n",
    "# 期待値E\n",
    "E_infer_z__x_logprob_gen_x__z = E(infer_nor_z__x, logprob_gen_x__z)\n",
    "\n",
    "# KLダイバージェンス\n",
    "KL_infer_nor_z__x_prior_nor_z = KullbackLeibler(infer_nor_z__x, prior_nor_z)\n",
    "\n",
    "# Lossの引き算\n",
    "total_loss = KL_infer_nor_z__x_prior_nor_z - E_infer_z__x_logprob_gen_x__z\n",
    "\n",
    "# Lossのmean\n",
    "total_loss = total_loss.mean()\n",
    "\n",
    "\n",
    "# Lossの確認\n",
    "print_latex(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$mean \\left(D_{KL} \\left[q(z|x)||p_{prior}(z) \\right] - \\mathbb{E}_{q(z|x)} \\left[\\log p(x|z) \\right] \\right)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl = KullbackLeibler(infer_nor_z__x, prior_nor_z)\n",
    "reconst = StochasticReconstructionLoss(infer_nor_z__x, gen_ber_x__z)\n",
    "vae_loss = (kl + reconst).mean()\n",
    "print_latex(vae_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModelAPIに確率分布とLossを渡し，最適化アルゴリズムを設定する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pixyz.modelsのModelを呼び出して使用\n",
    "主な引数はloss, distributions, optimizer, optimzer_paramsで，それぞれには以下のように格納します\n",
    "- loss: pixyz.lossesを使用して定義した目的関数のLossを格納\n",
    "- distributions: pixyz.distributionを使用して定義した，学習を行う確率分布を格納\n",
    "- optimizer, optimizer_params: 最適化アルゴリズム，そのパラメータを格納  \n",
    "\n",
    "For more details about Model: https://docs.pixyz.io/en/v0.0.4/_modules/pixyz/models/model.html#Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixyz.models import Model\n",
    "from torch import optim\n",
    "\n",
    "optimizer = optim.Adam\n",
    "optimizer_params = {'lr': 1e-3}\n",
    "\n",
    "vae_model = Model(loss=total_loss, \n",
    "                     distributions=[gen_ber_x__z, infer_nor_z__x],\n",
    "                     optimizer=optimizer,\n",
    "                     optimizer_params=optimizer_params\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上でModelの定義が完了した\n",
    "目的関数の設定と，最適化アルゴリズムの設定が独立に行えたことを確認できた\n",
    "次に実際にtrainメソッドについて確認し実際に学習を行う  \n",
    "Model Classのtrainメソッドでは以下の処理を行なっている  \n",
    "source code: https://docs.pixyz.io/en/v0.0.4/_modules/pixyz/models/model.html#Model.train\n",
    "1. 観測データであるxを受け取り(.train({\"x\": x}))\n",
    "2. Lossを計算し\n",
    "3. 1stepパラメーターの更新を行い\n",
    "4. Lossを出力  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def train(self, train_x={}, **kwargs):\n",
    "        self.distributions.train()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.loss_cls.estimate(train_x, **kwargs)\n",
    "\n",
    "        # backprop\n",
    "        loss.backward()\n",
    "\n",
    "        # update params\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  tensor(181.7224, grad_fn=<DivBackward0>)\n",
      "Epoch:  tensor(139.9781, grad_fn=<DivBackward0>)\n",
      "Epoch:  tensor(124.6789, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "epoch_loss = []\n",
    "for epoch in range(3):\n",
    "    train_loss = 0\n",
    "    for x, _ in train_loader:\n",
    "        x = x.to(device)\n",
    "        loss = vae_model.train({\"x\": x})\n",
    "        train_loss += loss\n",
    "    train_loss = train_loss * train_loader.batch_size / len(train_loader.dataset)\n",
    "    print('Epoch: ', train_loss)\n",
    "    epoch_loss.append(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上で学習を行えることを確認した  \n",
    "Pixyzでは高度なModelAPIとしてVAE, GAN Modelを用意しており，ただ入力データの変更やDNNのネットワークアーキテクチャーを変更したいだけの場合は高度なModel APIを使用することで簡単に実装することができる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高度なModel APIの使用\n",
    "- Pre-implementation models: https://docs.pixyz.io/en/v0.0.4/models.html#pre-implementation-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
