{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixyzの確率分布の記述方法\n",
    "\n",
    "ここではまず，Pixyzにおける確率モデルの実装方法について説明します.  \n",
    "Distribution API document: https://docs.pixyz.io/en/latest/distributions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11c5e06f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pixyz.utils import print_latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 深層ニューラルネットワークを用いない確率分布の定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 シンプルな確率分布の定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ガウス分布を作るためには，`Normal`をインポートして，平均（loc）と標準偏差（scale）を定義します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pixyz.distributions import Normal\n",
    "\n",
    "x_dim = 50\n",
    "p1_nor_x = Normal(loc=torch.tensor(0.), scale=torch.tensor(1.), var=['x'], features_shape=[x_dim], name='p_{1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なお``var``には，変数の名前を設定します．ここでは`\"x\"`を設定しています．\n",
    "\n",
    "また，features_shapeでは次元数を指定します．ここではfeatures_shapeが50となっていますから，50次元のサンプルを生成する形になります．\n",
    "\n",
    "上記で定義したp1の情報は次のようにみることができます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal\n",
      "p_{1}(x)\n"
     ]
    }
   ],
   "source": [
    "print(p1_nor_x.distribution_name) \n",
    "print(p1_nor_x.prob_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distribution_nameでは，確率分布の名前を確認できます．\n",
    "\n",
    "prob_textでは，確率分布の形をテキストで出力できます．ここでテキストに書かれている確率変数は，上記のvarで指定したものです.\n",
    "\n",
    "また，p1を丸ごとprintすると，以下のように表示されます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p_{1}(x)\n",
      "Network architecture:\n",
      "  Normal(\n",
      "    name=p_{1}, distribution_name=Normal,\n",
      "    var=['x'], cond_var=[], input_var=[], features_shape=torch.Size([50])\n",
      "    (loc): torch.Size([1, 50])\n",
      "    (scale): torch.Size([1, 50])\n",
      "  )\n"
     ]
    }
   ],
   "source": [
    "print(p1_nor_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print_latexを利用するとLaTex表記で定義した確率分布が表示されます  \n",
    "注: 数式のtex形式への出力に外部ライブラリのsympy使用しており，sympyの影響で数式の結果に支障を与えないが，数式の順序が入れ替わることがある  \n",
    "print_latex(A +B)の出力が  \n",
    "B+Aになることがあったりする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$p_{1}(x)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_latex(p1_nor_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に，定義した分布からサンプリングしてみましょう． サンプリングは，`sample()`によって実行します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091,\n",
      "         -0.7121,  0.3037, -0.7773, -0.2515, -0.2223,  1.6871,  0.2284,  0.4676,\n",
      "         -0.6970, -1.1608,  0.6995,  0.1991,  0.8657,  0.2444, -0.6629,  0.8073,\n",
      "          1.1017, -0.1759, -2.2456, -1.4465,  0.0612, -0.6177, -0.7981, -0.1316,\n",
      "          1.8793, -0.0721,  0.0663, -0.4370,  0.7626,  0.4415,  1.1651,  2.0154,\n",
      "          0.2152, -0.5242, -0.1860, -0.6446,  1.5392, -0.8696, -3.3312, -0.7479,\n",
      "          1.1173,  0.2981]])}\n",
      "tensor([[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091,\n",
      "         -0.7121,  0.3037, -0.7773, -0.2515, -0.2223,  1.6871,  0.2284,  0.4676,\n",
      "         -0.6970, -1.1608,  0.6995,  0.1991,  0.8657,  0.2444, -0.6629,  0.8073,\n",
      "          1.1017, -0.1759, -2.2456, -1.4465,  0.0612, -0.6177, -0.7981, -0.1316,\n",
      "          1.8793, -0.0721,  0.0663, -0.4370,  0.7626,  0.4415,  1.1651,  2.0154,\n",
      "          0.2152, -0.5242, -0.1860, -0.6446,  1.5392, -0.8696, -3.3312, -0.7479,\n",
      "          1.1173,  0.2981]])\n"
     ]
    }
   ],
   "source": [
    "p1_nor_x_samples = p1_nor_x.sample()\n",
    "print(p1_nor_x_samples)\n",
    "print(p1_nor_x_samples[\"x\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出力はdict形式になっています．\n",
    "\n",
    "サンプリング結果を確認したい変数について指定することで，中身を確認できます（ただし，この例では変数は\"x\"のみです）．\n",
    "\n",
    "なお，サンプリング結果は，PyTorchのtensor形式になっています．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 条件付確率分布の定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に条件付確率分布の定義の仕方を正規分布の例で見ていきます\n",
    "\n",
    "正規分布ではパラメータは平均$\\mu$と分散$\\sigma^2$がありますが，今回は平均が条件付けられた正規分布を取り上げたいと思います"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(x|\\mu_{var}) = \\cal N(x; \\mu=\\mu_{var}, \\sigma^2=1)$\n",
    "\n",
    "分布の条件付き変数の設定はcond_varで行います  \n",
    "ここではmu_varという変数を正規分布の平均に設定したいため  \n",
    "cond_var=['mu_var']  \n",
    "loc='mu_var'  \n",
    "とします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_dim = 50\n",
    "p1_nor_x__mu = Normal(loc='mu_var', scale=torch.tensor(1.), var=['x'], cond_var=['mu_var'], features_shape=[x_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p(x|\\mu_{var})\n",
      "Network architecture:\n",
      "  Normal(\n",
      "    name=p, distribution_name=Normal,\n",
      "    var=['x'], cond_var=['mu_var'], input_var=['mu_var'], features_shape=torch.Size([50])\n",
      "    (scale): torch.Size([1, 50])\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$p(x|\\mu_{var})$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(p1_nor_x__mu)\n",
    "print_latex(p1_nor_x__mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで平均が$\\mu_{var}$で条件付けされる正規分布が定義できました  \n",
    "試しに$\\mu_{var}=0$としてxをサンプリングしてみます  \n",
    "sampleメソッドにdict形式で変数を指定します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mu_var': 0,\n",
       " 'x': tensor([[-0.5962, -1.0055, -0.2106, -0.0075,  1.6734,  0.0103,  0.9837,  0.8793,\n",
       "          -0.9962, -0.8313, -0.4610, -0.5601,  0.3956, -0.9823,  1.3264,  0.8547,\n",
       "          -0.6540,  0.7317, -1.4344, -0.5008,  0.1716, -0.1600, -0.5047, -1.4746,\n",
       "          -1.0412,  0.7323, -1.0483, -0.4709,  0.2911,  1.9907, -0.9247, -0.9301,\n",
       "           0.8165, -0.9135,  0.2053,  0.3051,  0.5357, -0.4312,  0.1573,  1.2540,\n",
       "           1.3275, -0.4954, -1.9804,  1.7986,  0.1018,  0.3400, -0.6447, -0.2870,\n",
       "           3.3212, -0.4021]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1_nor_x__mu.sample({\"mu_var\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に$\\mu_{var}$自体も何らかの確率分布に従う変数とし，その確率分布を定めます  \n",
    "ここでは仮にベルヌーイ分布とします  \n",
    "$p(\\mu_{var}) = \\cal B(\\mu_{var};p=0.3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pixyz.distributions import Bernoulli\n",
    "p2_ber_mu = Bernoulli(probs=torch.tensor(0.3), var=['mu_var'], features_shape=[x_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p(\\mu_{var})\n",
      "Network architecture:\n",
      "  Bernoulli(\n",
      "    name=p, distribution_name=Bernoulli,\n",
      "    var=['mu_var'], cond_var=[], input_var=[], features_shape=torch.Size([50])\n",
      "    (probs): torch.Size([1, 50])\n",
      "  )\n",
      "{'mu_var': tensor([[0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])}\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$p(\\mu_{var})$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(p2_ber_mu)\n",
    "print(p2_ber_mu.sample())\n",
    "print_latex(p2_ber_mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pixyzでは分布の積は，掛け算で表すことができます  \n",
    "定義した$p(\\mu_{var})$と$p(x|\\mu_{var})$を掛け合わせて同時分布$p(x, \\mu_{var})$を定義します  \n",
    "$p(x, \\mu_{var}) = p(x|\\mu_{var}) p(\\mu_{var})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p(x,\\mu_{var}) = p(x|\\mu_{var})p(\\mu_{var})\n",
      "Network architecture:\n",
      "  Bernoulli(\n",
      "    name=p, distribution_name=Bernoulli,\n",
      "    var=['mu_var'], cond_var=[], input_var=[], features_shape=torch.Size([50])\n",
      "    (probs): torch.Size([1, 50])\n",
      "  )\n",
      "  Normal(\n",
      "    name=p, distribution_name=Normal,\n",
      "    var=['x'], cond_var=['mu_var'], input_var=['mu_var'], features_shape=torch.Size([50])\n",
      "    (scale): torch.Size([1, 50])\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$p(x,\\mu_{var}) = p(x|\\mu_{var})p(\\mu_{var})$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_joint_mu_x = p1_nor_x__mu * p2_ber_mu\n",
    "print(p_joint_mu_x) \n",
    "print_latex(p_joint_mu_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "同時分布でも今までと同様にsampleメソッドでサンプリングを行うことができます  \n",
    "全ての変数とその値がdict形式で出力されます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mu_var': tensor([[1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
       "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1.]]),\n",
       " 'x': tensor([[ 3.6415, -0.9624,  0.7924, -1.3889,  1.0127, -0.8734,  1.7997,  1.2824,\n",
       "           1.6604,  0.2717,  0.1913,  0.1267,  0.5707,  0.8652,  0.3437,  0.3718,\n",
       "           0.1444,  1.7772, -2.3381,  0.1709,  1.1661,  1.4787,  0.2676,  0.7561,\n",
       "          -0.5873, -2.0619,  0.4305,  0.3377, -0.3438, -0.6172,  2.2530, -0.0514,\n",
       "          -1.0257,  0.5213, -2.3065,  1.6037,  0.1794,  0.1447,  0.6411,  0.4793,\n",
       "           0.7617, -0.3542, -0.2693,  2.3120, -0.8920, -0.7529, -0.0573,  2.2000,\n",
       "           0.9912,  0.9414]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_joint_mu_x.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 深層ニューラルネットワークと組み合わせた確率分布の設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に， 確率分布のパラメータを深層ニューラルネットワークで定義します．\n",
    "\n",
    "例えば，ガウス分布の平均$\\mu$と分散$\\sigma^2$は， パラメータ$\\theta$を持つ深層ニューラルネットワークによって，$\\mu=f(x;\\theta)$および$\\sigma^2=g(x;\\theta)$と定義できます．\n",
    "\n",
    "したがって，ガウス分布は${\\cal N}(\\mu=f(x;\\theta),\\sigma^2=g(x;\\theta))$となります．\n",
    "\n",
    "$p(a) = {\\cal N}(a; \\mu=f(x;\\theta),\\sigma^2=g(x;\\theta))$を定義してみましょう\n",
    "\n",
    "Pixyzでは，次のようなクラスを記述することで，これを実現できます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a_dim = 20\n",
    "\n",
    "class ProbNorAgivX(Normal):\n",
    "    \"\"\"\n",
    "    Probability distrituion Normal A given X\n",
    "    p(a) = {\\cal N}(a; \\mu=f(x;\\theta),\\sigma^2=g(x;\\theta)\n",
    "    loc and scale are parameterized by theta given x\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ProbNorAgivX, self).__init__(cond_var=['x'], var=['a'])\n",
    "        \n",
    "        self.fc1 = nn.Linear(x_dim, 10)\n",
    "        self.fc_loc = nn.Linear(10, a_dim)\n",
    "        self.fc_scale = nn.Linear(10, a_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return {'loc': self.fc_loc(h1), 'scale': F.softplus(self.fc_scale(h1))}\n",
    "p_nor_a__x = ProbNorAgivX()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず， ガウス分布クラスを継承することで，ガウス分布のパラメータを深層ニューラルネットワークで定義することを明示します．\n",
    "\n",
    "次に，コンストラクタで，利用するニューラルネットワークを記述します．これは，通常のPyTorchと同じです．\n",
    "\n",
    "唯一異なる点は，superの引数にvarとcond_varの名前を指定している点です．\n",
    "\n",
    "varは先程見たように，出力する変数の名前を指定します．一方，cond_varではニューラルネットワークの入力変数の名前を指定します．これは，ここで定義する分布において，条件付けられる変数とみなすことができます．\n",
    "\n",
    "forwardについても，通常のPyTorchと同じです．ただし，注意点が2つあります．\n",
    "\n",
    "* 引数の名前と数は，cond_varで設定したものと同じにしてください． 例えば，cond_var=[\"x\", \"y\"]とした場合は，forward(self, x, y)としてください．ただし，引数の順番は変えても構いません．->引数の順番は変えても構いません．どういうこと\n",
    "* 戻り値は，それぞれの確率分布のパラメータになります．上記の例ではガウス分布なので，平均と分散を指定しています．\n",
    "\n",
    "そして最後に，定義した確率分布クラスのインスタンスを作成します．\n",
    "\n",
    "次に，先程の例と同様，確率分布の情報を見てみましょう."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p(a|x)\n",
      "Network architecture:\n",
      "  ProbNorAgivX(\n",
      "    name=p, distribution_name=Normal,\n",
      "    var=['a'], cond_var=['x'], input_var=['x'], features_shape=torch.Size([])\n",
      "    (fc1): Linear(in_features=50, out_features=10, bias=True)\n",
      "    (fc_loc): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (fc_scale): Linear(in_features=10, out_features=20, bias=True)\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$p(a|x)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(p_nor_a__x)\n",
    "print_latex(p_nor_a__x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p2の分布は，xで条件付けた形になっています．これらの表記は，superの引数で設定したとおりになっています．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に，先程の例のように，サンプリングしてみましょう．\n",
    "\n",
    "注意しなければならないのは，先ほどと異なり，条件づけた変数xがあるということです．\n",
    "\n",
    "x_samplesをxとしてサンプリングしましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_samples = torch.Tensor([[-0.3030, -1.7618,  0.6348, -0.8044, -1.0371, -1.0669, -0.2085,\n",
    "         -0.2155,  2.2952,  0.6749,  1.7133, -1.7943, -1.5208,  0.9196,\n",
    "         -0.5484, -0.3472,  0.4730, -0.4286,  0.5514, -1.5474,  0.7575,\n",
    "         -0.4068, -0.1277,  0.2804,  1.7460,  1.8550, -0.7064,  2.5571,\n",
    "          0.7705, -1.0739, -0.2015, -0.5603, -0.6240, -0.9773, -0.1637,\n",
    "         -0.3582, -0.0594, -2.4919,  0.2423,  0.2883, -0.1095,  0.3126,\n",
    "         -0.3417,  0.9473,  0.6223, -0.4481, -0.2856,  0.3880, -1.1435,\n",
    "         -0.6512]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([[-0.3030, -1.7618,  0.6348, -0.8044, -1.0371, -1.0669, -0.2085, -0.2155,\n",
      "          2.2952,  0.6749,  1.7133, -1.7943, -1.5208,  0.9196, -0.5484, -0.3472,\n",
      "          0.4730, -0.4286,  0.5514, -1.5474,  0.7575, -0.4068, -0.1277,  0.2804,\n",
      "          1.7460,  1.8550, -0.7064,  2.5571,  0.7705, -1.0739, -0.2015, -0.5603,\n",
      "         -0.6240, -0.9773, -0.1637, -0.3582, -0.0594, -2.4919,  0.2423,  0.2883,\n",
      "         -0.1095,  0.3126, -0.3417,  0.9473,  0.6223, -0.4481, -0.2856,  0.3880,\n",
      "         -1.1435, -0.6512]]), 'a': tensor([[-1.7231e-01, -5.0856e-01,  1.3573e+00, -7.1246e-01,  3.8644e-01,\n",
      "          1.1225e+00,  1.4864e-01,  6.8819e-02, -5.6884e-01, -2.4427e+00,\n",
      "          1.2279e-03, -9.0337e-01,  5.3217e-02,  6.0509e-01, -3.8033e-01,\n",
      "          6.5707e-02, -2.3049e-01,  3.4607e-01,  2.6745e-02, -3.9659e-01]])}\n",
      "tensor([[-1.7231e-01, -5.0856e-01,  1.3573e+00, -7.1246e-01,  3.8644e-01,\n",
      "          1.1225e+00,  1.4864e-01,  6.8819e-02, -5.6884e-01, -2.4427e+00,\n",
      "          1.2279e-03, -9.0337e-01,  5.3217e-02,  6.0509e-01, -3.8033e-01,\n",
      "          6.5707e-02, -2.3049e-01,  3.4607e-01,  2.6745e-02, -3.9659e-01]])\n",
      "tensor([[-0.3030, -1.7618,  0.6348, -0.8044, -1.0371, -1.0669, -0.2085, -0.2155,\n",
      "          2.2952,  0.6749,  1.7133, -1.7943, -1.5208,  0.9196, -0.5484, -0.3472,\n",
      "          0.4730, -0.4286,  0.5514, -1.5474,  0.7575, -0.4068, -0.1277,  0.2804,\n",
      "          1.7460,  1.8550, -0.7064,  2.5571,  0.7705, -1.0739, -0.2015, -0.5603,\n",
      "         -0.6240, -0.9773, -0.1637, -0.3582, -0.0594, -2.4919,  0.2423,  0.2883,\n",
      "         -0.1095,  0.3126, -0.3417,  0.9473,  0.6223, -0.4481, -0.2856,  0.3880,\n",
      "         -1.1435, -0.6512]])\n"
     ]
    }
   ],
   "source": [
    "p_nor_a__x_samples = p_nor_a__x.sample({'x': x_samples})\n",
    "print(p_nor_a__x_samples)\n",
    "print(p_nor_a__x_samples['a'])\n",
    "print(p_nor_a__x_samples['x'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出力には，aとxの２つのサンプルがあります．\n",
    "\n",
    "aが今回計算したサンプルで，xについては，引数として与えたサンプルがそのまま入っています．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Tutorial\n",
    "02-LossAPITutorial.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
