{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In deep generative models, model design means defining objective functions\n",
    "- Any deep generative models explicitly set the objective function to optimize\n",
    "    - Autoregressive models・Flow models: Kullback-Leibler divergence(log likelihood)\n",
    "    - VAE: Evidence lower bound\n",
    "    - GAN: Jensen-Shannon divergence(GAN also needs update of objective function itsself(=adversarial learning))\n",
    "- Regularization terms of inference or random variable representation is incorporated in the objective function\n",
    "<img src='../tutorial_figs/vae_loss.png'>\n",
    "   \n",
    "    - In deep generative models, model design means defining objective functions\n",
    "    - Unlike traditional generative models, deep generative models don't inference by sampling\n",
    "- A framework that receives probability distributions and defines the objective functions\n",
    "    - LossAPI  \n",
    "<img src='../tutorial_figs/PixyzAPI.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receive probability distribution and define the objective function\n",
    "- Loss API document: https://pixyz.readthedocs.io/en/latest/losses.html#\n",
    "\n",
    "We take probability distributions defined in Distribution API and define the objective function  \n",
    "In order to define the objective function, it needs these elements.  \n",
    "1. calculate likelihood\n",
    "1. calculate the distance between probability distribution\n",
    "1. calculate the expected value\n",
    "1. calculation considering data distribution(mean, sum)  \n",
    "\n",
    "In VAE loss, each elements corresponds as follows\n",
    "<img src='../tutorial_figs/vae_loss_API.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the loss\n",
    "Loss API needs input variable(`input_var`). The value of loss is calculated not until the input variable feeds into the loss\n",
    "```python\n",
    "p = DistributionAPI()\n",
    "# define the objective function receiving distribution\n",
    "loss = LossAPI(p)\n",
    "# the value of loss is calculated when input_var is feeded\n",
    "loss_value = loss.eval({'input_var': input_data})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x119fd76d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pixyz module\n",
    "from pixyz.distributions import Normal\n",
    "from pixyz.utils import print_latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate likelihood\n",
    "When the observation $x_1$, ...., $x_N$ is obtained, we calculate the likelihood of the probability distribution p, which we assume x follows  \n",
    "Here, we assume x follows a gaussian distribution with mean=0, variance = 1  \n",
    "$p(x) = \\cal N(\\mu=0, \\sigma^2=1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p(x)\n",
      "Network architecture:\n",
      "  Normal(\n",
      "    name=p, distribution_name=Normal,\n",
      "    var=['x'], cond_var=[], input_var=[], features_shape=torch.Size([5])\n",
      "    (loc): torch.Size([1, 5])\n",
      "    (scale): torch.Size([1, 5])\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$p(x)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define probability distribution p\n",
    "x_dim = 5\n",
    "p_nor_x = Normal(var=['x'], loc=torch.tensor(0.), scale=torch.tensor(1.), features_shape=[x_dim])\n",
    "print(p_nor_x)\n",
    "print_latex(p_nor_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 5])\n"
     ]
    }
   ],
   "source": [
    "# observe x\n",
    "observed_x_num = 100\n",
    "observed_x = torch.randn(observed_x_num, x_dim)\n",
    "print(observed_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log likelihood is calculated as follows  \n",
    "$L=\\sum_{i=1}^{100} \\log p\\left(x_{i}\\right)$  \n",
    "We can calculate log likelihood easily by using LogProb()  \n",
    "To define log likelihood, We set the probability distribution defined in Pixyz distribution to LogProb()'s argument.  \n",
    "The value is calculated when observed data feeded into LogProb.eval()  \n",
    "Pixyz document: https://docs.pixyz.io/en/latest/losses.html#probability-density-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\log p(x)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pixyz.losses import LogProb\n",
    "# set the probability distribution to LogProb()'s arg\n",
    "log_likelihood_x = LogProb(p_nor_x)\n",
    "print_latex(log_likelihood_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -7.5539,  -6.8545,  -6.4024,  -5.8851,  -6.1517,  -8.3702,  -6.7028,\n",
      "         -5.0395,  -7.4346,  -7.1497,  -5.7594,  -7.3006, -11.9857,  -5.8238,\n",
      "         -6.7561,  -5.7640,  -6.2382,  -4.9060,  -6.1076,  -8.2535,  -7.8250,\n",
      "         -7.1956,  -7.6949,  -5.2324, -11.5860,  -8.1068,  -7.1763,  -8.3332,\n",
      "        -11.4631,  -6.6297,  -6.1200, -12.2358,  -5.3402,  -7.1465,  -7.5106,\n",
      "         -7.0829,  -6.6300,  -6.1832,  -7.2049, -10.8676,  -6.8674,  -5.8339,\n",
      "         -9.1939,  -7.5965,  -8.7743,  -7.3492,  -5.2578, -10.3097,  -6.5646,\n",
      "         -4.8807,  -5.9738,  -6.2394, -10.3945,  -9.1760,  -9.2957,  -5.5627,\n",
      "         -7.1047,  -6.4066,  -6.8100,  -6.0878,  -6.8835,  -7.9132,  -5.0738,\n",
      "         -8.8378,  -6.2286,  -5.8401,  -5.9691,  -5.6857,  -7.6903,  -6.4982,\n",
      "         -7.1259,  -8.7953, -10.5572,  -5.9161,  -7.0649,  -6.1292,  -6.0871,\n",
      "         -7.2513,  -7.2517,  -7.1378,  -6.4228,  -5.5728,  -5.6155,  -5.1962,\n",
      "         -8.3940,  -7.8178,  -9.8129,  -6.1119,  -5.0492,  -8.9898,  -6.9675,\n",
      "         -8.0218, -13.9816,  -6.8575,  -5.1304,  -5.5069,  -5.0561,  -5.1264,\n",
      "         -4.8489,  -5.4876])\n",
      "observed_x_num:  100\n"
     ]
    }
   ],
   "source": [
    "# The likelihood for each observation is calculated\n",
    "print(log_likelihood_x.eval({'x': observed_x}))\n",
    "# observed_x_num = 100\n",
    "print('observed_x_num: ', len(log_likelihood_x.eval({'x': observed_x})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log_likelihood_x.eval({'x': observed_x})'s output has the calculated result of  \n",
    "$\\log p(x_{1})$, $\\log p(x_{2})$, ...., $\\log p(x_{100})$  \n",
    "\n",
    "log_likelihood_x.eval({'x': observed_x})[i] = $\\log p(x_{i})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, calculate  \n",
    "$L=\\sum_{i=1}^{100} \\log p\\left(x_{i}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood result: tensor(-715.5875)\n"
     ]
    }
   ],
   "source": [
    "# sum\n",
    "print('log likelihood result:', log_likelihood_x.eval({'x': observed_x}).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, we can easily calculate log likelihood by using pixyz.losses LogProb()  \n",
    "The same calculation can be performed by defined probability distribution method `p.log_prob().eval()`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogProb()\n",
      "tensor(-715.5875)\n",
      ".log_prob()\n",
      "tensor(-715.5875)\n"
     ]
    }
   ],
   "source": [
    "print('LogProb()')\n",
    "print(LogProb(p_nor_x).eval({'x': observed_x}).sum())\n",
    "print('.log_prob()')\n",
    "print(p_nor_x.log_prob().eval({'x': observed_x}).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more Loss API related to probability density function:  \n",
    "https://docs.pixyz.io/en/latest/losses.html#probability-density-function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the distance between probability distributions\n",
    "In the learning of generative models, we consider $p_{\\theta}(x)$ that is closed to the true distribution(data distribution) $p_{data}(x)$.  \n",
    "To find the appropriate parameter $\\theta$, we measure the distance between distributions.\n",
    "\n",
    "In VAE models we calculate Kullback-Leibler divergence, and in GAN models we calculate Jensen-Shannon divergence.  \n",
    "We can easily calculte the distance between distributions by Loss API  \n",
    "Pixyz document:  \n",
    "https://docs.pixyz.io/en/latest/losses.html#statistical-distance  \n",
    "https://pixyz.readthedocs.io/en/latest/losses.html#adversarial-statistical-distance\n",
    "\n",
    "Here, we calculate the Kullback-Leibler divergence between a gaussian distribution with mean=0, variance=1 and a gaussian distribution with mean=5, variance=0.1  \n",
    "$p(x) = \\cal N(\\mu=0, \\sigma^2=1)$  \n",
    "$q(x) = \\cal N(\\mu=5, \\sigma^2=0.1)$  \n",
    "$KL(q(x) || p(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$p(x)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define probability distribution\n",
    "x_dim = 10\n",
    "# p \n",
    "p_nor_x = Normal(var=['x'], loc=torch.tensor(0.), scale=torch.tensor(1.), features_shape=[x_dim])\n",
    "print_latex(p_nor_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$q(x)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q\n",
    "q_nor_x = Normal(var=['x'], loc=torch.tensor(5.), scale=torch.tensor(0.1), features_shape=[x_dim], name='q')\n",
    "print_latex(q_nor_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate Kullback-Leibler divergence, we use pixyz.losses `KullbackLeibler`.  \n",
    "We set the probability distribution defined in Pixyz distribution to `KullbackLeibler()`'s argument.  \n",
    "The value is calculated by `.eval()` method    \n",
    "Pixyz document: https://docs.pixyz.io/en/latest/losses.html#kullbackleibler  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$D_{KL} \\left[q(x)||p(x) \\right]$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pixyz.losses import KullbackLeibler\n",
    "\n",
    "kl_q_p = KullbackLeibler(q_nor_x, p_nor_x)\n",
    "print_latex(kl_q_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([143.0759])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculte the value\n",
    "kl_q_p.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more Loss API related to statistical distance: \n",
    "https://docs.pixyz.io/en/latest/losses.html#statistical-distance  \n",
    "https://docs.pixyz.io/en/latest/losses.html#adversarial-statistical-distance  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 期待値を計算する\n",
    "何らかの関数について確率分布で重み付けして積分を行うのが期待値計算であるが\n",
    "Pixyzでは潜在変数のように, input_varとして与えられない変数がある場合その変数が従う確率分布で潰\n",
    "期待値の計算もLoss APIを用いれば簡単に計算できる  \n",
    "Pixyz document:  \n",
    "https://docs.pixyz.io/en/latest/losses.html#expected-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは例として  \n",
    "$q(z|x) = \\cal N(\\mu=x, \\sigma^2=1)$  \n",
    "$p(x|z) = \\cal N(\\mu=z, \\sigma^2=1)$  \n",
    "といった二つの確率分布q(z|x), p(x|z)を考え  \n",
    "$\\mathbb{E}_{q(z|x)} \\left[\\log p(x|z) \\right]$を計算する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 確率分布の定義\n",
    "from pixyz.distributions import Normal\n",
    "\n",
    "q_nor_z__x = Normal(loc=\"x\", scale=torch.tensor(1.), var=[\"z\"], cond_var=[\"x\"],\n",
    "           features_shape=[10], name='q') # q(z|x)\n",
    "p_nor_x__z = Normal(loc=\"z\", scale=torch.tensor(1.), var=[\"x\"], cond_var=[\"z\"],\n",
    "                    features_shape=[10]) # p(x|z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\log p(x|z)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p(x|z)の対数尤度をとる\n",
    "from pixyz.losses import LogProb\n",
    "\n",
    "p_log_likelihood = LogProb(p_nor_x__z)\n",
    "print_latex(p_log_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "期待値の計算はpixyz.lossesのExpectationを用いる  \n",
    "Expectation()の引数にはp, fがあり  \n",
    "期待値をとる対象の関数がfで, その関数の確率変数が従う確率分布のpで重み付けを行う  \n",
    ".eval()で計算が行われる  \n",
    "Pixyz document: https://docs.pixyz.io/en/latest/losses.html#expected-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\mathbb{E}_{q(z|x)} \\left[\\log p(x|z) \\right]$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pixyz.losses import Expectation as E\n",
    "\n",
    "E_q_logprob_p = E(q_nor_z__x, LogProb(p_nor_x__z))\n",
    "print_latex(E_q_logprob_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-10.7006, -11.9861])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_x = torch.randn(2, 10)\n",
    "E_q_logprob_p.eval({'x': sample_x}) # どういうこと？？どういう状況なのか？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details about Expectatoin API:  \n",
    "https://docs.pixyz.io/en/latest/losses.html#expected-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ分布を考慮した計算(mean, sum)\n",
    "本来ならxについて期待値をとる必要があるが，データ分布は実際に与えられないためbatch方向について平均や合計といった計算を行う  \n",
    "合計や平均といった計算もLoss APIでは簡単に行うことができる  \n",
    "ここではobserved_xを訓練データとして尤度計算を行いそのmeanを計算する\n",
    "$p(x) = \\cal N(\\mu=0, \\sigma^2=1)$  \n",
    "$\\frac{1}{N} \\sum_{i=1}^N\\left[\\log p\\left(x^{(i)}\\right)\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 5])\n"
     ]
    }
   ],
   "source": [
    "# xを観測\n",
    "observed_x_num = 100\n",
    "x_dim = 5\n",
    "observed_x = torch.randn(observed_x_num, x_dim)\n",
    "print(observed_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "  p(x)\n",
      "Network architecture:\n",
      "  Normal(\n",
      "    name=p, distribution_name=Normal,\n",
      "    var=['x'], cond_var=[], input_var=[], features_shape=torch.Size([5])\n",
      "    (loc): torch.Size([1, 5])\n",
      "    (scale): torch.Size([1, 5])\n",
      "  )\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$p(x)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 確率分布pを定義\n",
    "p_nor_x = Normal(var=['x'], loc=torch.tensor(0.), scale=torch.tensor(1.), features_shape=[x_dim])\n",
    "print(p_nor_x)\n",
    "print_latex(p_nor_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合計や平均といった計算はLoss.mean()やLoss.sum()とすることで容易に行える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$mean \\left(\\log p(x) \\right)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pixyz.losses import LogProb\n",
    "# meanを計算する\n",
    "mean_log_likelihood_x = LogProb(p_nor_x).mean() # .mean()\n",
    "print_latex(mean_log_likelihood_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-7.1973)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_log_likelihood_x.eval({'x': observed_x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lossの組み合わせ\n",
    "PixyzではLoss同士の四則演算ができる  \n",
    "例として以下のLossをLoss同士の組み合わせで表現する  \n",
    "$\\frac{1}{N} \\sum_{i=1}^{N}\\left[\\mathbb{E}_{q\\left(z | x^{(i)}\\right)}\\left[\\log p\\left(x^{(i)} | z\\right)\\right]-K L\\left(q\\left(z | x^{(i)}\\right) \\| p(z)\\right)\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 確率分布の定義\n",
    "from pixyz.distributions import Normal\n",
    "\n",
    "# p(x|z)\n",
    "p_nor_x__z = Normal(loc=\"z\", scale=torch.tensor(1.), var=[\"x\"], cond_var=[\"z\"],\n",
    "                    features_shape=[10])\n",
    "\n",
    "# p(z)\n",
    "p_nor_z = Normal(loc=torch.tensor(0.), scale=torch.tensor(1.), var=[\"z\"],\n",
    "                    features_shape=[10])\n",
    "\n",
    "# q(z|x)\n",
    "q_nor_z__x = Normal(loc=\"x\", scale=torch.tensor(1.), var=[\"z\"], cond_var=[\"x\"],\n",
    "           features_shape=[10], name='q')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: input var\n",
    "    fがx, zで\n",
    "    pがzの確率分分布だったらfの対応するinput_varが一つなくなる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$mean \\left(- D_{KL} \\left[q(z|x)||p(z) \\right] + \\mathbb{E}_{q(z|x)} \\left[\\log p(x|z) \\right] \\right)$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lossの定義\n",
    "from pixyz.losses import LogProb\n",
    "from pixyz.losses import Expectation as E\n",
    "from pixyz.losses import KullbackLeibler\n",
    "\n",
    "# 対数尤度\n",
    "logprob_p_x__z = LogProb(p_nor_x__z)\n",
    "\n",
    "# 期待値E\n",
    "E_q_z__x_logprob_p__z = E(q_nor_z__x, logprob_p_x__z)\n",
    "\n",
    "# KLダイバージェンス\n",
    "KL_q_z__x_p_z = KullbackLeibler(q_nor_z__x, p_nor_z)\n",
    "\n",
    "# Lossの引き算\n",
    "total_loss = E_q_z__x_logprob_p__z - KL_q_z__x_p_z\n",
    "\n",
    "# Lossのmean\n",
    "total_loss = total_loss.mean()\n",
    "\n",
    "# Lossの確認\n",
    "print_latex(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-18.9965)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lossの計算\n",
    "# xを観測\n",
    "observed_x_num = 100\n",
    "x_dim = 10\n",
    "observed_x = torch.randn(observed_x_num, x_dim)\n",
    "\n",
    "# 観測したxのLossを計算\n",
    "total_loss.eval({'x': observed_x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上のようにPixyz Loss API同士の四則演算で柔軟にLossが定義でき，数式から実装までが直感的に行えることが確認できた"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss API(ELBO)\n",
    "Pixyz Loss APIでは以下のようなLossについても実装がある\n",
    "\n",
    "周辺尤度下界 ELBO: https://docs.pixyz.io/en/latest/losses.html#lower-bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Tutorial\n",
    "ModelAPITutorial.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
